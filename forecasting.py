import marimo

__generated_with = "0.19.11"
app = marimo.App(width="medium")


@app.cell
def _():
    import marimo as mo
    import os
    import gc
    import psutil
    import warnings
    import logging
    import numpy as np
    import polars as pl
    import lightgbm as lgb
    import xgboost as xgb

    # Suppress warnings
    warnings.filterwarnings("ignore")
    logging.getLogger("cmdstanpy").disabled = True
    logging.getLogger("pytorch_lightning").setLevel(logging.ERROR)

    # Configuration for Memory Management
    pl.Config.set_streaming_chunk_size(1000)

    # Check for Darts and Torch (Deep Learning)
    try:
        import torch

        # Optimize for 30-series GPUs (Ampere)
        torch.set_float32_matmul_precision("medium")

        # Try importing Statistical Baselines (Prophet, AutoARIMA)
        try:
            import importlib.util

            if importlib.util.find_spec("statsforecast") is None:
                raise ImportError
        except ImportError:
            print(
                "‚ö†Ô∏è StatsForecast/Prophet not found. Install 'statsforecast' and 'prophet' for statistical baselines."
            )

        print("‚úÖ Darts & Torch available for Deep Learning.")
    except ImportError:
        print(
            "‚ö†Ô∏è Darts/Torch not found. Deep Learning & Statistical baselines will be skipped."
        )
        print(
            "To enable, run: pip install darts torch pytorch-lightning prophet statsforecast"
        )
    return gc, lgb, mo, np, os, pl, psutil, torch, xgb


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    # üìà Advanced Time-Series Forecasting

    This notebook implements a multi-stage forecasting pipeline for the Kaggle `ts-forecasting` competition.
    It is optimized for limited hardware (16GB RAM, 8GB VRAM) by using:

    1.  **Polars** for memory-efficient data manipulation.
    2.  **Gradient Boosting** (LightGBM/XGBoost) with GPU acceleration for the core engine.
    3.  **Deep Learning** (N-BEATS/TFT via Darts) for capturing complex non-linear patterns.
    4.  **Statistical Models** (AutoARIMA/Prophet) for robust baselines.
    """)
    return


@app.cell
def _(gc, os, pl, psutil):
    # --- Utilities ---
    def get_memory_usage():
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024

    def free_memory():
        gc.collect()
        try:
            import torch

            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except ImportError:
            pass
        except Exception:
            pass

    def optimize_types(df):
        """Downcast types to save memory."""
        # Handle LazyFrame or DataFrame
        is_lazy = isinstance(df, pl.LazyFrame)
        schema = df.collect_schema() if is_lazy else df.schema

        exprs = []
        for name, dtype in schema.items():
            if name == "id":
                continue
            if dtype == pl.Float64:
                exprs.append(pl.col(name).cast(pl.Float32))
            elif dtype == pl.Int64:
                exprs.append(pl.col(name).cast(pl.Int32))

        if exprs:
            return df.with_columns(exprs)
        return df

    print(f"Initial Memory Usage: {get_memory_usage():.2f} MB")

    # Paths
    DATA_DIR = "data"
    TRAIN_PATH = os.path.join(DATA_DIR, "train.parquet")
    TEST_PATH = os.path.join(DATA_DIR, "test.parquet")

    if not os.path.exists(TRAIN_PATH):
        raise FileNotFoundError(
            f"Data not found at {TRAIN_PATH}. Please ensure data is in the 'data' directory."
        )
    return TEST_PATH, TRAIN_PATH, free_memory


@app.cell
def _(TEST_PATH, TRAIN_PATH, mo, pl):
    mo.md("## 1. Data Loading & Preprocessing (Optimized)")

    # Load and optimize using Lazy API
    print("Loading data (Lazy)...")
    tr_lf = pl.scan_parquet(TRAIN_PATH)
    te_lf = pl.scan_parquet(TEST_PATH)

    # Get max_ts for splitting
    max_ts = tr_lf.select(pl.col("ts_index").max()).collect().item()
    print(f"Max TS in Train: {max_ts}")

    # Configuration
    GROUP_COLS = ["code", "sub_code", "sub_category"]

    # Pre-process Test to ensure we have needed horizons/ts_index
    # We will use this to join features later
    return GROUP_COLS, max_ts, te_lf, tr_lf


@app.cell
def _(GROUP_COLS, free_memory, mo, np, pl, tr_lf):
    mo.md(
        r"""
    ## 2. Advanced Feature Engineering

    1.  **Hierarchical Trends**: Lagged means for `code` and `sub_category`.
    2.  **Momentum**: Difference between short and long lags.
    3.  **Cyclical Time**: sin/cos encoding for day of week/month.
    """
    )

    # Config
    LAGS = [1, 7, 14, 28]
    WINDOWS = [7, 28]

    # --- Advanced Feature Engineering ---

    # 1. Target Denoising (Winsorization)
    tr_denoised_lf = tr_lf.with_columns(
        pl.col("y_target")
        .clip(
            tr_lf.select(pl.col("y_target").quantile(0.001)).collect().item(),
            tr_lf.select(pl.col("y_target").quantile(0.999)).collect().item(),
        )
        .alias("y_target_denoised")
    )

    # --- Hierarchical Aggregations (Capture market-wide category shifts) ---
    cat_agg = tr_denoised_lf.group_by(["sub_category", "ts_index"]).agg(
        pl.col("y_target_denoised").mean().alias("cat_y_mean")
    )
    code_agg = tr_denoised_lf.group_by(["code", "ts_index"]).agg(
        pl.col("y_target_denoised").mean().alias("code_y_mean")
    )

    # Base Series Logic
    series_lf = tr_denoised_lf.sort(GROUP_COLS + ["ts_index"])
    series_lf = series_lf.join(cat_agg, on=["sub_category", "ts_index"], how="left")
    series_lf = series_lf.join(code_agg, on=["code", "ts_index"], how="left")

    # --- Cross-Sectional Signals (The "Alpha" Features) ---
    feature_exprs = []

    # Cyclical Time
    feature_exprs.extend(
        [
            (np.sin(2 * np.pi * (pl.col("ts_index") % 7) / 7.0)).alias("dow_sin"),
            (np.cos(2 * np.pi * (pl.col("ts_index") % 7) / 7.0)).alias("dow_cos"),
            (np.sin(2 * np.pi * (pl.col("ts_index") % 30) / 30.0)).alias("dom_sin"),
            (np.cos(2 * np.pi * (pl.col("ts_index") % 30) / 30.0)).alias("dom_cos"),
        ]
    )

    # Series Lags
    for lag in LAGS:
        feature_exprs.append(
            pl.col("y_target_denoised").shift(lag).over(GROUP_COLS).alias(f"lag_{lag}")
        )

    for w in WINDOWS:
        feature_exprs.append(
            pl.col("y_target_denoised")
            .shift(1)
            .rolling_mean(w)
            .over(GROUP_COLS)
            .alias(f"roll_mean_{w}")
        )

    # Hierarchical Signals (Restore the category averages)
    feature_exprs.append(
        pl.col("cat_y_mean").shift(1).over(GROUP_COLS).alias("cat_lag_1")
    )
    feature_exprs.append(
        pl.col("code_y_mean").shift(1).over(GROUP_COLS).alias("code_lag_1")
    )

    # Momentum
    feature_exprs.append(
        (pl.col("y_target_denoised").shift(1) - pl.col("y_target_denoised").shift(7))
        .over(GROUP_COLS)
        .alias("momentum_7d")
    )

    # Recency Weighting
    max_ts_val = tr_lf.select(pl.col("ts_index").max()).collect().item()

    # Pass 1: Signal Matrix
    series_with_features = series_lf.with_columns(feature_exprs)

    # Pass 2: Ranking (Stability)
    rank_exprs = []
    for lag in LAGS:
        rank_exprs.append(
            pl.col(f"lag_{lag}")
            .rank("dense")
            .over(["sub_category", "ts_index"])
            .alias(f"lag_{lag}_cat_rank")
        )

    series_with_features = series_with_features.with_columns(rank_exprs).fill_null(0.0)
    series_with_features = series_with_features.with_columns(
        (pl.col("weight") * ((pl.col("ts_index") - max_ts_val) / 180.0).exp()).alias(
            "v_weight"
        )
    )

    series_with_features = series_with_features.with_columns(
        [
            pl.col(c).cast(pl.Float32)
            for c in series_with_features.collect_schema().names()
            if series_with_features.collect_schema()[c] == pl.Float64
        ]
    )

    feature_cols = [
        c
        for c in series_with_features.collect_schema().names()
        if any(
            x in c
            for x in ["lag_", "roll_", "do", "momentum", "_rank", "cat_lag", "code_lag"]
        )
    ]

    full_series_features = series_with_features.select(
        ["id"]
        + GROUP_COLS
        + ["ts_index", "y_target", "y_target_denoised", "v_weight", "weight"]
        + feature_cols
    )

    free_memory()
    print(f"Refined Feature Engineering Ready. Features: {len(feature_cols)}")
    return feature_cols, full_series_features


@app.cell
def _(
    GROUP_COLS,
    feature_cols,
    free_memory,
    full_series_features,
    lgb,
    max_ts,
    mo,
    np,
    pl,
    te_lf,
    torch,
    xgb,
):
    mo.md("## 3. Training & Prediction Pipeline")

    # Split cutoff for local validation
    split_cutoff = int(max_ts * 0.9)

    print("Materializing Train/Valid Data...")

    print("Materializing Training Data (Raw-Target)...")
    train_data = full_series_features.filter(
        pl.col("ts_index") < split_cutoff
    ).collect()
    X_train = train_data.select(feature_cols).to_numpy()
    y_train = train_data.select("y_target").to_series().to_numpy()
    w_train = train_data.select("v_weight").to_series().to_numpy()

    print(f"X_train shape: {X_train.shape}")
    del train_data
    free_memory()

    # Valid: Use remaining historical data
    valid_data = full_series_features.filter(
        (pl.col("ts_index") >= split_cutoff) & (pl.col("ts_index") <= max_ts)
    ).collect()
    X_valid = valid_data.select(feature_cols).to_numpy()
    y_valid = valid_data.select("y_target").to_series().to_numpy()
    w_valid = valid_data.select("weight").to_series().to_numpy()
    valid_ids = valid_data.select("id").to_series().to_list()

    # Track extra info for validation breakdown
    try:
        h_valid = valid_data.select("horizon").to_series().to_numpy()
    except Exception:
        h_valid = np.ones(len(y_valid))  # Dummy if not present

    print(f"X_valid shape: {X_valid.shape}")
    del valid_data
    free_memory()

    # --- Test Data Construction (The CRITICAL Fix) ---
    print("Constructing Test Data with Multi-Horizon Mapping...")
    # Map test row (t_target, horizon) to features at (t_target - horizon + 1)

    # 1. Prepare test dataframe with mapping key
    test_mapped = te_lf.with_columns(
        (pl.col("ts_index") - pl.col("horizon") + 1)
        .cast(pl.Int32)
        .alias("feature_ts_index")
    )

    # 2. Join with pre-computed series features
    # We join features based on the calculated feature_ts_index
    # Ensure ts_index is Int32 for the join
    test_with_features = (
        test_mapped.join(
            full_series_features.with_columns(pl.col("ts_index").cast(pl.Int32)).select(
                GROUP_COLS + ["ts_index"] + feature_cols
            ),
            left_on=GROUP_COLS + ["feature_ts_index"],
            right_on=GROUP_COLS + ["ts_index"],
            how="left",
        )
        .fill_null(0.0)
        .fill_nan(0.0)
        .collect()
    )

    X_test = test_with_features.select(feature_cols).to_numpy()
    test_ids = test_with_features.select("id").to_series().to_list()

    print(f"X_test shape: {X_test.shape}")
    del test_with_features
    free_memory()

    # --- Training ---
    use_gpu = torch.cuda.is_available()
    print(f"GPU Available: {use_gpu}")

    # LGBM (Regression/MSE)
    print("Training LightGBM (Regression)...")
    lgb_params = {
        "n_estimators": 2000,
        "learning_rate": 0.02,
        "num_leaves": 63,
        "device": "gpu" if use_gpu else "cpu",
        "objective": "regression",
        "metric": "rmse",
        "verbose": -1,
        "subsample": 0.6,
        "colsample_bytree": 0.6,
        "max_bin": 63,
    }
    model_lgb = lgb.LGBMRegressor(**lgb_params)
    model_lgb.fit(
        X_train,
        y_train,
        sample_weight=w_train,
        eval_set=[(X_valid, y_valid)],
        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],
    )
    pred_lgb_val = model_lgb.predict(X_valid)
    pred_lgb_test = model_lgb.predict(X_test)
    del model_lgb
    free_memory()

    # XGBoost (Regression/MSE)
    print("Training XGBoost (Regression)...")
    xgb_params = {
        "n_estimators": 2000,
        "learning_rate": 0.02,
        "max_depth": 6,
        "tree_method": "hist",
        "device": "cuda" if use_gpu else "cpu",
        "objective": "reg:squarederror",
        "early_stopping_rounds": 50,
        "max_bin": 63,
        "subsample": 0.6,
        "colsample_bytree": 0.6,
    }

    model_xgb = xgb.XGBRegressor(**xgb_params)
    model_xgb.fit(
        X_train,
        y_train,
        sample_weight=w_train,
        eval_set=[(X_valid, y_valid)],
        verbose=False,
    )
    pred_xgb_val = model_xgb.predict(X_valid)
    pred_xgb_test = model_xgb.predict(X_test)
    del model_xgb
    free_memory()
    return (
        h_valid,
        pred_lgb_test,
        pred_lgb_val,
        pred_xgb_test,
        pred_xgb_val,
        test_ids,
        valid_ids,
        w_valid,
        y_valid,
    )


@app.cell
def _(mo, np, pred_lgb_test, pred_lgb_val, pred_xgb_test, pred_xgb_val):
    mo.md("## 4. Ensembling & Submission Prep")

    # Weighted Average Ensemble
    ensemble_weights = [0.5, 0.5]

    final_pred_val = (
        ensemble_weights[0] * pred_lgb_val + ensemble_weights[1] * pred_xgb_val
    )

    final_pred_test = (
        ensemble_weights[0] * pred_lgb_test + ensemble_weights[1] * pred_xgb_test
    )

    # Clip predictions to non-negative
    final_pred_test = np.maximum(final_pred_test, 0)
    final_pred_val = np.maximum(final_pred_val, 0)

    print("Ensemble Predictions Generated.")
    return final_pred_test, final_pred_val


@app.cell
def _(final_pred_val, h_valid, mo, np, w_valid, y_valid):
    mo.md(
        r"""
    ## 5. Local Leaderboard Simulation

    This cell simulates the Kaggle scoring environment. We split our validation holdout into two sets:
    1.  **Public Score (25%)**: Mimics the real-time feedback.
    2.  **Private Score (75%)**: Mimics the final final evaluation.
    """
    )

    def official_skill_score(y_target, y_pred, w) -> float:
        """Official formula from evaluate.py: sqrt(1 - clip(SSE_w / SST_w))"""
        denom = np.sum(w * (y_target**2))
        if denom == 0:
            return 0.0
        numerator = np.sum(w * ((y_target - y_pred) ** 2))
        ratio = numerator / denom
        return float(np.sqrt(np.maximum(1.0 - ratio, 0.0)))

    # Simulate Public/Private Split
    indices = np.arange(len(y_valid))
    np.random.seed(42)  # For reproducibility
    np.random.shuffle(indices)

    split_idx = int(len(y_valid) * 0.25)
    public_idx, private_idx = indices[:split_idx], indices[split_idx:]

    score_public = official_skill_score(
        y_valid[public_idx], final_pred_val[public_idx], w_valid[public_idx]
    )
    score_private = official_skill_score(
        y_valid[private_idx], final_pred_val[private_idx], w_valid[private_idx]
    )
    overall = official_skill_score(y_valid, final_pred_val, w_valid)

    print("-" * 65)
    print(f"{'LOCAL LEADERBOARD (SIMULATED)':<35} | {'SCORE':<20}")
    print("-" * 65)
    print(f"{'üî¥ Simulated Public LB (25%)':<35} | {score_public:.6f}")
    print(f"{'üîí Simulated Private LB (75%)':<35} | {score_private:.6f}")
    print(f"{'üìä Overall Holdout Score':<35} | {overall:.6f}")
    print("-" * 65)

    # Horizon Breakdown
    unique_h = np.unique(h_valid)
    if len(unique_h) > 1:
        print("\nScores by Horizon:")
        print(f"  {'Horizon':<10} | {'Score':<10} | {'Samples':<10}")
        for h in sorted(unique_h):
            mask = h_valid == h
            if mask.any():
                h_score = official_skill_score(
                    y_valid[mask], final_pred_val[mask], w_valid[mask]
                )
                print(f"  {int(h):<10} | {h_score:.6f}   | {mask.sum():,}")
    return


@app.cell
def _(final_pred_test, final_pred_val, mo, pl, test_ids, valid_ids):
    mo.md("## 6. Submission")

    submission_df = pl.DataFrame({"id": test_ids, "prediction": final_pred_test})

    submission_path = "submission_optimized.csv"
    submission_df.write_csv(submission_path)
    print(f"Submission saved to: {submission_path}")

    # --- Validation Export (For GitHub History Accuracy) ---
    val_df = pl.DataFrame({"id": valid_ids, "prediction": final_pred_val})
    val_path = "validation_results.csv"
    val_df.write_csv(val_path)
    print(f"Validation results saved to: {val_path}")
    return


if __name__ == "__main__":
    app.run()
