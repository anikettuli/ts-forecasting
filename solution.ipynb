{
  "cells": [
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 0: Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import requests\n",
        "\n",
        "def download_data(url, filepath):\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"File {filepath} already exists. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Downloading data from {url}...\")\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    \n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(filepath, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"Downloaded to {filepath}\")\n",
        "    else:\n",
        "        print(f\"Failed to download. Status code: {response.status_code}\")\n",
        "\n",
        "DATA_URL = \"https://storage.googleapis.com/kagglesdsdata/competitions/105581/15271735/test.parquet?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1770992767&Signature=Fj%2F5LPgyVbzbph1WeG3uCJRMdqsabiq%2BKM3msHzs8y%2BXoHuAzoKKjvFDg3t8ueiPngilQtsFWg4FbUfzajf%2BDqzehOTGwRWzU6bX0xvdrrOeQusa7glpDDnF9n6C0izwzU8k%2FxFDdU7qI6vUtJLm3Yk20zfZMYx%2BuGtFrrtoTzcUx0k5ut%2Ft4OtyyBeCzpsUpCA5EjKMGbqRnB5P%2F7SCEWlwCQ7ZXL8w0kKJGCC3%2FYOqSktMDRhaeLsyP3lfCejur%2BxGfcd8hoLQWsEHOkYEw91k%2F%2BOy6vg5PkpYlQN2Exovmjg3o56VBIAZLLZhU%2BCAvtfL4X%2Bw02HrNJVKi5mhoA%3D%3D&response-content-disposition=attachment%3B+filename%3Dtest.parquet\"\n",
        "download_data(DATA_URL, \"data/test.parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports & Metric Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "import warnings\n",
        "import os\n",
        "import lightgbm as lgb\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from typing import Tuple, List, Dict\n",
        "import requests\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Define the Weighted RMSE Score\n",
        "def weighted_rmse_score(\n",
        "    y_true: np.ndarray, y_pred: np.ndarray, weights: np.ndarray\n",
        ") -> float:\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    weights = np.asarray(weights)\n",
        "\n",
        "    # Calculate weighted RMSE numerator and denominator\n",
        "    weighted_squared_error = np.sum(weights * (y_true - y_pred) ** 2)\n",
        "    weighted_y_squared = np.sum(weights * y_true**2)\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if weighted_y_squared == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate skill score\n",
        "    score = 1 - np.sqrt(weighted_squared_error / weighted_y_squared)\n",
        "    return score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteration A: Load Data & Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Iteration A: The Golden Split & Metric (Polars)\n",
        "\n",
        "def load_and_split_data(\n",
        "    filepath: str = \"data/test.parquet\",\n",
        "    target_col: str = \"feature_ch\",\n",
        "    weight_col: str = \"feature_cg\",\n",
        "    valid_ratio: float = 0.25,\n",
        ") -> Tuple[pl.DataFrame, pl.DataFrame, list]:\n",
        "    print(\"Loading data from\", filepath)\n",
        "    if not os.path.exists(filepath):\n",
        "        # Create dummy data for testing if file doesn't exist\n",
        "        print(\"Warning: Data file not found. Creating dummy data.\")\n",
        "        n_rows = 10000\n",
        "        df = pl.DataFrame({\n",
        "            \"id\": [f\"c_sc_cat_h_{i}\" for i in range(n_rows)],\n",
        "            \"ts_index\": np.arange(n_rows),\n",
        "            \"code\": np.random.choice([\"A\", \"B\"], n_rows).tolist(),\n",
        "            \"sub_code\": np.random.choice([\"X\", \"Y\"], n_rows).tolist(),\n",
        "            \"sub_category\": np.random.choice([\"1\", \"2\"], n_rows).tolist(),\n",
        "            \"horizon\": np.random.choice([1, 10, 25], n_rows).tolist(),\n",
        "            \"feature_ch\": np.random.randn(n_rows).tolist(),  # Target\n",
        "            \"feature_cg\": np.random.uniform(0.5, 1.5, n_rows).tolist(), # Weight\n",
        "        })\n",
        "        # Add dummy features\n",
        "        for i in range(10):\n",
        "            df = df.with_columns(pl.lit(np.random.randn(n_rows)).alias(f\"feature_{i}\"))\n",
        "    else:\n",
        "        df = pl.read_parquet(filepath)\n",
        "    \n",
        "    print(f\"Loaded {df.height:,} rows with {len(df.columns)} columns\")\n",
        "\n",
        "    # Determine split point based on ts_index\n",
        "    min_ts = df[\"ts_index\"].min()\n",
        "    max_ts = df[\"ts_index\"].max()\n",
        "    ts_range = max_ts - min_ts\n",
        "    split_ts = max_ts - int(ts_range * valid_ratio)\n",
        "\n",
        "    print(f\"Time index range: {min_ts} to {max_ts}\")\n",
        "    print(f\"Validation split at ts_index >= {split_ts}\")\n",
        "\n",
        "    # Split data using filter (lazy or eager)\n",
        "    train_df = df.filter(pl.col(\"ts_index\") < split_ts)\n",
        "    valid_df = df.filter(pl.col(\"ts_index\") >= split_ts)\n",
        "    \n",
        "    # Feature columns (exclude meta)\n",
        "    exclude_cols = [\"id\", \"code\", \"sub_code\", \"sub_category\", target_col, weight_col, \"ts_index\", \"horizon\"]\n",
        "    feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "\n",
        "    return train_df, valid_df, feature_cols\n",
        "\n",
        "# Execute Iter A\n",
        "train_df, valid_df, feature_cols = load_and_split_data()\n",
        "\n",
        "# Baseline Calculation\n",
        "y_true = valid_df[\"feature_ch\"].to_numpy()\n",
        "weights = valid_df[\"feature_cg\"].fill_null(1.0).to_numpy() # Polars handles nulls differently\n",
        "y_pred_baseline = np.ones_like(y_true) * train_df[\"feature_ch\"].mean()\n",
        "baseline_score = weighted_rmse_score(y_true, y_pred_baseline, weights)\n",
        "print(f\"Baseline (Mean Prediction) Score: {baseline_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteration B: Temporal Features (Polars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Iteration B: Temporal Feature Engineering (Polars)\n",
        "# Polars makes rolling windows extremely fast and clean.\n",
        "\n",
        "def create_temporal_features_pl(\n",
        "    df: pl.DataFrame,\n",
        "    feature_cols: List[str],\n",
        "    group_cols: List[str] = [\"code\", \"sub_code\", \"sub_category\"],\n",
        "    rolling_windows: List[int] = [3, 7, 14, 30],\n",
        ") -> pl.DataFrame:\n",
        "    print(\"Creating temporal features with Polars...\")\n",
        "    \n",
        "    # Ensure sorted by ts_index within groups\n",
        "    df = df.sort(group_cols + [\"ts_index\"])\n",
        "    \n",
        "    features_to_process = feature_cols[:20] if len(feature_cols) > 50 else feature_cols\n",
        "    \n",
        "    # Define expressions for rolling/lag features\n",
        "    exprs = []\n",
        "    \n",
        "    for feat in features_to_process:\n",
        "        # Lags\n",
        "        for lag in [1, 2, 3]:\n",
        "            exprs.append(pl.col(feat).shift(lag).over(group_cols).alias(f\"{feat}_lag_{lag}\"))\n",
        "            \n",
        "        # Rolling Means (Shifted by 1 to prevent leakage)\n",
        "        # Polars rolling operates on the column. .shift(1) ensures we don't peek.\n",
        "        for window in rolling_windows:\n",
        "            # rolling_mean usage: .rolling_mean(window_size).shift(1)\n",
        "            # We must use .over(group_cols) to respect groups!\n",
        "            # However, rolling_mean is deprecated in favor of rolling().mean()\n",
        "            # To apply over groups efficiently:\n",
        "            # We can use window functions inside over()\n",
        "            \n",
        "            # Note: naive .rolling() inside over() can be tricky in older Polars versions, \n",
        "            # but newer versions support it well.\n",
        "            # Best practice: use creating rolling columns separately if over() is complex or use window functions.\n",
        "            \n",
        "            # Efficient pattern: \n",
        "            # (col(feat).shift(1).rolling_mean(window)).over(group_cols)\n",
        "            # Shift FIRST to prevent leakage, then roll. Wait.\n",
        "            # If we shift first, then rolling window at T includes T-1, T-2... which is safe.\n",
        "            # Yes.\n",
        "            \n",
        "            col_name = f\"{feat}_roll_{window}\"\n",
        "            exprs.append(\n",
        "                pl.col(feat)\n",
        "                .shift(1)\n",
        "                .rolling_mean(window_size=window, min_periods=1)\n",
        "                .over(group_cols)\n",
        "                .alias(col_name)\n",
        "            )\n",
        "            \n",
        "        # Expanding Mean (Shifted)\n",
        "        # Cumulative sum / Count\n",
        "        # Shift(1) first\n",
        "        shifted = pl.col(feat).shift(1)\n",
        "        exprs.append(\n",
        "            (shifted.cum_sum() / shifted.cum_count())\n",
        "            .over(group_cols)\n",
        "            .alias(f\"{feat}_exp_mean\")\n",
        "            .fill_nan(0) # Handle potential division by zero\n",
        "        )\n",
        "\n",
        "    # Apply all expressions at once! Ultra fast.\n",
        "    df = df.with_columns(exprs)\n",
        "    \n",
        "    print(f\"Created {len(exprs)} temporal features\")\n",
        "    return df\n",
        "\n",
        "# Combine\n",
        "full_df = pl.concat([train_df, valid_df])\n",
        "\n",
        "# Create Features\n",
        "full_df = create_temporal_features_pl(full_df, feature_cols)\n",
        "\n",
        "# Re-split\n",
        "# We need to calculate split_ts again or reuse\n",
        "split_ts = full_df[\"ts_index\"].max() - int((full_df[\"ts_index\"].max() - full_df[\"ts_index\"].min()) * 0.25)\n",
        "train_df = full_df.filter(pl.col(\"ts_index\") < split_ts)\n",
        "valid_df = full_df.filter(pl.col(\"ts_index\") >= split_ts)\n",
        "\n",
        "# Update feature list\n",
        "current_features = [c for c in full_df.columns if c not in [\"id\", \"code\", \"sub_code\", \"sub_category\", \"feature_ch\", \"feature_cg\", \"ts_index\", \"horizon\"]]\n",
        "print(f\"Total features after Iter B: {len(current_features)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteration C: Weighted LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Iteration C: Weighted LightGBM (Polars -> Numpy)\n",
        "\n",
        "def train_lgb_model(train_df, valid_df, features, params=None):\n",
        "    if params is None:\n",
        "        params = {\n",
        "            \"objective\": \"regression\",\n",
        "            \"metric\": \"rmse\",\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"num_leaves\": 31,\n",
        "            \"verbose\": -1,\n",
        "            \"n_jobs\": -1,\n",
        "            \"device\": \"gpu\"\n",
        "        }\n",
        "    \n",
        "    # Convert to numpy/pandas for LightGBM consumption (LightGBM supports Polars directly in newer versions too!)\n",
        "    # But for safety and consistency with weights, we'll extract explicitly.\n",
        "    \n",
        "    X_train = train_df.select(features).fill_null(0).to_numpy()\n",
        "    y_train = train_df[\"feature_ch\"].to_numpy()\n",
        "    w_train = train_df[\"feature_cg\"].fill_null(1.0).to_numpy()\n",
        "    \n",
        "    X_valid = valid_df.select(features).fill_null(0).to_numpy()\n",
        "    y_valid = valid_df[\"feature_ch\"].to_numpy()\n",
        "    w_valid = valid_df[\"feature_cg\"].fill_null(1.0).to_numpy()\n",
        "    \n",
        "    train_data = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
        "    valid_data = lgb.Dataset(X_valid, label=y_valid, weight=w_valid, reference=train_data)\n",
        "    \n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=1000,\n",
        "        valid_sets=[train_data, valid_data],\n",
        "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(False)]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Train separate models for horizons\n",
        "horizons = sorted(train_df[\"horizon\"].unique().to_list())\n",
        "models = {}\n",
        "\n",
        "print(\"Training Horizon-specific Models (Iter C)...\")\n",
        "for h in horizons:\n",
        "    t_h = train_df.filter(pl.col(\"horizon\") == h)\n",
        "    v_h = valid_df.filter(pl.col(\"horizon\") == h)\n",
        "    \n",
        "    if t_h.height == 0 or v_h.height == 0: continue\n",
        "        \n",
        "    model = train_lgb_model(t_h, v_h, current_features)\n",
        "    models[h] = model\n",
        "    \n",
        "    # Score\n",
        "    v_preds = model.predict(v_h.select(current_features).fill_null(0).to_numpy())\n",
        "    score = weighted_rmse_score(v_h[\"feature_ch\"].to_numpy(), v_preds, v_h[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "    print(f\"  Horizon {h} Score: {score:.4f}\")\n",
        "\n",
        "# Overall Evaluation C\n",
        "# We can join predictions back\n",
        "valid_df = valid_df.with_columns(pl.lit(0.0).alias(\"pred_iter_c\"))\n",
        "\n",
        "# Since we trained separate models, we iterate to update\n",
        "# Polars update is immutable-ish. We can use map/when/then but loop is easier with to_pandas/numpy?\n",
        "# Actually, let's keep predictions separate and join or update efficiently.\n",
        "preds_full = []\n",
        "for h, model in models.items():\n",
        "    mask = valid_df[\"horizon\"] == h\n",
        "    # Filter\n",
        "    sub_df = valid_df.filter(pl.col(\"horizon\") == h)\n",
        "    if sub_df.height > 0:\n",
        "        preds = model.predict(sub_df.select(current_features).fill_null(0).to_numpy())\n",
        "        # We need to map these back. \n",
        "        # Easier strategy: collect predictions with index/ID then join.\n",
        "        temp_df = sub_df.select(\"id\").with_columns(pl.Series(name=\"pred_iter_c_h\", values=preds))\n",
        "        preds_full.append(temp_df)\n",
        "\n",
        "if preds_full:\n",
        "    preds_all = pl.concat(preds_full)\n",
        "    valid_df = valid_df.join(preds_all, on=\"id\", how=\"left\").with_columns(\n",
        "        pl.col(\"pred_iter_c_h\").fill_null(0).alias(\"pred_iter_c\")\n",
        "    )\n",
        "\n",
        "overall_score_c = weighted_rmse_score(\n",
        "    valid_df[\"feature_ch\"].to_numpy(), \n",
        "    valid_df[\"pred_iter_c\"].to_numpy(), \n",
        "    valid_df[\"feature_cg\"].fill_null(1.0).to_numpy()\n",
        ")\n",
        "print(f\"Overall Iteration C Score: {overall_score_c:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteration D: PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Iteration D: PCA (Polars -> Sklearn)\n",
        "\n",
        "print(\"Applying PCA (Iter D)...\")\n",
        "\n",
        "# Select numeric features\n",
        "pca_features = [c for c in train_df.columns if c.startswith(\"feature_\") or \"_roll_\" in c]\n",
        "pca_features = pca_features[:50]\n",
        "\n",
        "# To Numpy for PCA\n",
        "X_train_np = train_df.select(pca_features).fill_null(0).to_numpy()\n",
        "X_valid_np = valid_df.select(pca_features).fill_null(0).to_numpy()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_np)\n",
        "X_valid_scaled = scaler.transform(X_valid_np)\n",
        "\n",
        "n_components=10\n",
        "pca = PCA(n_components=n_components)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_valid_pca = pca.transform(X_valid_scaled)\n",
        "\n",
        "# Add back to Polars\n",
        "# We create a dataframe of PCA feats and horizontal stack\n",
        "pca_cols = [f\"pca_{i}\" for i in range(n_components)]\n",
        "train_pca_df = pl.DataFrame(X_train_pca, schema=pca_cols)\n",
        "valid_pca_df = pl.DataFrame(X_valid_pca, schema=pca_cols)\n",
        "\n",
        "train_df = pl.concat([train_df, train_pca_df], how=\"horizontal\")\n",
        "valid_df = pl.concat([valid_df, valid_pca_df], how=\"horizontal\")\n",
        "\n",
        "features_d = current_features + pca_cols\n",
        "# ... code for model training D (abbreviated, same pattern) ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteration E: Smoothed Target Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Iteration E: Smoothed Target Encoding (Polars Fast)\n",
        "\n",
        "def create_smoothed_target_encoding_pl(\n",
        "    df, col, target=\"feature_ch\", weight=\"feature_cg\", min_samples=10, smoothing=10\n",
        "):\n",
        "    # Sort\n",
        "    df = df.sort([col, \"ts_index\"])\n",
        "    global_mean = df[target].mean()\n",
        "    \n",
        "    # Polars expressions for expanding mean\n",
        "    # We want: (cumsum_shift * n_shift + smooth*global) / (n_shift + smooth)\n",
        "    \n",
        "    # Calculate Expanding Sum and Count\n",
        "    # Use over()\n",
        "    \n",
        "    return df.with_columns(\n",
        "        (\n",
        "            (pl.col(target).shift(1).cum_sum().over(col).fill_null(0) + smoothing * global_mean) \n",
        "            / \n",
        "            (pl.col(target).shift(1).cum_count().over(col).fill_null(0) + smoothing)\n",
        "        ).alias(f\"{col}_enc_smooth\")\n",
        "    )\n",
        "\n",
        "print(\"Applying Smoothed Target Encoding (Iter E)...\")\n",
        "full_df = pl.concat([train_df.select(pl.exclude(pca_cols)), valid_df.select(pl.exclude(pca_cols))])\n",
        "# Need to re-gen PCA or just concat all? Reuse existing.\n",
        "# Let's just concat all columns carefully.\n",
        "# Actually, straightforward concat works if columns match.\n",
        "full_df = pl.concat([train_df, valid_df])\n",
        "\n",
        "for col in [\"code\", \"sub_code\", \"sub_category\"]:\n",
        "    full_df = create_smoothed_target_encoding_pl(full_df, col)\n",
        "\n",
        "# Re-split\n",
        "train_df = full_df.filter(pl.col(\"ts_index\") < split_ts)\n",
        "valid_df = full_df.filter(pl.col(\"ts_index\") >= split_ts)\n",
        "\n",
        "features_e = features_d + [f\"{c}_enc_smooth\" for c in [\"code\", \"sub_code\", \"sub_category\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteration F: Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Iteration F: Interaction Features (Polars)\n",
        "\n",
        "print(\"Creating Interaction Features (Iter F)...\")\n",
        "# Polars expression API makes this trivial\n",
        "new_cols = []\n",
        "base_feats = [c for c in features_e if \"lag\" in c][:5]\n",
        "\n",
        "for feat in base_feats:\n",
        "    new_cols.append((pl.col(feat) * pl.col(\"horizon\")).alias(f\"{feat}_x_horizon\"))\n",
        "\n",
        "new_cols.append((pl.col(\"horizon\") ** 2).alias(\"horizon_squared\"))\n",
        "\n",
        "train_df = train_df.with_columns(new_cols)\n",
        "valid_df = valid_df.with_columns(new_cols)\n",
        "\n",
        "interaction_feats = [c.name for c in train_df.select(new_cols)] # Get names? alias sets name.\n",
        "# Re-extract names\n",
        "interaction_feats = [f\"{feat}_x_horizon\" for feat in base_feats] + [\"horizon_squared\"]\n",
        "\n",
        "all_candidates_f = features_e + interaction_feats\n",
        "\n",
        "# Feature Selection (Sklearn)\n",
        "X_train_np = train_df.select(all_candidates_f).fill_null(0).to_numpy()\n",
        "y_train_np = train_df[\"feature_ch\"].to_numpy()\n",
        "\n",
        "selector = SelectKBest(score_func=f_regression, k=min(100, len(all_candidates_f)))\n",
        "selector.fit(X_train_np, y_train_np)\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "selected_features_f = [all_candidates_f[i] for i in selected_indices]\n",
        "print(f\"Selected {len(selected_features_f)} features.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteration G: Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Iteration G: Ensemble\n",
        "\n",
        "print(\"Training Ensemble (Iter G)...\")\n",
        "\n",
        "horizons = sorted(train_df[\"horizon\"].unique().to_list())\n",
        "param_sets = [\n",
        "    {\"num_leaves\": 31, \"learning_rate\": 0.05, \"bagging_fraction\": 0.8},\n",
        "    {\"num_leaves\": 63, \"learning_rate\": 0.03, \"bagging_fraction\": 0.9},\n",
        "]\n",
        "\n",
        "preds_all_g = []\n",
        "\n",
        "for h in horizons:\n",
        "    t_h = train_df.filter(pl.col(\"horizon\") == h)\n",
        "    v_h = valid_df.filter(pl.col(\"horizon\") == h)\n",
        "    if t_h.height == 0: continue\n",
        "    \n",
        "    horizon_preds = []\n",
        "    for p in param_sets:\n",
        "        full_params = {\"objective\": \"regression\", \"metric\": \"rmse\", \"verbose\": -1, \"n_jobs\": -1, \"device\": \"gpu\", **p}\n",
        "        model = train_lgb_model(t_h, v_h, selected_features_f, params=full_params)\n",
        "        preds = model.predict(v_h.select(selected_features_f).fill_null(0).to_numpy())\n",
        "        horizon_preds.append(preds)\n",
        "    \n",
        "    avg_preds = np.mean(horizon_preds, axis=0)\n",
        "    \n",
        "    # Store with ID\n",
        "    temp_df = v_h.select(\"id\").with_columns(pl.Series(\"prediction\", avg_preds))\n",
        "    preds_all_g.append(temp_df)\n",
        "\n",
        "if preds_all_g:\n",
        "    submission = pl.concat(preds_all_g)\n",
        "    submission.write_csv(\"submission_final_polars.csv\")\n",
        "    print(\"Saved submission_final_polars.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}