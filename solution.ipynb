{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 0: Download Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# GPU-Accelerated Time Series Forecasting\n",
        "\n",
        "**Hardware**: RTX 3070 + CUDA 13.1 + CuPy 13.3.0 + LightGBM\n",
        "\n",
        "**7-Step Pipeline**:\n",
        "1. Load data & baseline metric\n",
        "2. Temporal features (lags, rolling windows)\n",
        "3. Horizon-specific LightGBM models\n",
        "4. PCA dimensionality reduction\n",
        "5. Smoothed target encoding\n",
        "6. Feature selection & interactions\n",
        "7. Ensemble predictions\n",
        "\n",
        "**Key Design**: Matrix operations (GPU) â†’ CuPy. External libs (CPU) â†’ NumPy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: /usr/bin/python3\n",
            "CuPy Version: 13.6.0\n",
            "âœ“ GPU Ready: 1 device(s), Compute Capability 75\n"
          ]
        }
      ],
      "source": [
        "# GPU Setup & Initialization\n",
        "\n",
        "import cupy as np\n",
        "import numpy as np_cpu\n",
        "import sys\n",
        "\n",
        "print(f\"Python: {sys.executable}\")\n",
        "print(f\"CuPy Version: {np.__version__}\")\n",
        "\n",
        "# Check GPU\n",
        "try:\n",
        "    device_count = np.cuda.runtime.getDeviceCount()\n",
        "    device = np.cuda.Device(0)\n",
        "    cap = device.compute_capability\n",
        "    print(f\"âœ“ GPU Ready: {device_count} device(s), Compute Capability {cap}\")\n",
        "except Exception as e:\n",
        "    print(f\"âœ— GPU Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Step 1: Download Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data for competition 'ts-forecasting'...\n",
            "Failed to download via Kaggle API: Command '['kaggle', 'competitions', 'download', '-c', 'ts-forecasting']' returned non-zero exit status 1.\n",
            "Ensure 'kaggle' is installed and credentials are set.\n"
          ]
        }
      ],
      "source": [
        "# Kaggle Data Download\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def download_kaggle_data(competition_name=\"ts-forecasting\"):\n",
        "    data_dir = \"data\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "    \n",
        "    # Check if files already exist\n",
        "    if os.path.exists(os.path.join(data_dir, \"train.parquet\")) and \\\n",
        "       os.path.exists(os.path.join(data_dir, \"test.parquet\")):\n",
        "        print(\"Data files already exist. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Downloading data for competition '{competition_name}'...\")\n",
        "    try:\n",
        "        # Colab/Linux: Ensure .kaggle directory exists for the CLI\n",
        "        root_kaggle = os.path.expanduser(\"~/.kaggle\")\n",
        "        if not os.path.exists(root_kaggle):\n",
        "            os.makedirs(root_kaggle)\n",
        "\n",
        "        # Use the specific Token variable required for KGAT_ tokens\n",
        "        # This fixes the '401 Unauthorized' and 'Exit Status 1' errors\n",
        "        subprocess.run([\n",
        "            \"kaggle\", \"competitions\", \"download\", \"-c\", competition_name\n",
        "        ], check=True, env=os.environ)\n",
        "        \n",
        "        # Unzip\n",
        "        import zipfile\n",
        "        zip_path = f\"{competition_name}.zip\"\n",
        "        if os.path.exists(zip_path):\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(data_dir)\n",
        "            os.remove(zip_path)\n",
        "            print(f\"Downloaded and extracted to {data_dir}/\")\n",
        "        else:\n",
        "            print(\"Zip file not found. Check Kaggle API output.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download via Kaggle API: {e}\")\n",
        "        print(\"Tip: If in Colab, ensure you have accepted competition rules on Kaggle website.\")\n",
        "\n",
        "# Correct credentials for the new API Token format\n",
        "os.environ[\"KAGGLE_API_TOKEN\"] = \"KGAT_ccc00b322d3c4b85f0036a23cc420469\"\n",
        "\n",
        "download_kaggle_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Step 2: Imports & Metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utilities & Metric\n",
        "\n",
        "import polars as pl\n",
        "import warnings\n",
        "import os\n",
        "import lightgbm as lgb\n",
        "import cupy as np\n",
        "import numpy as np_cpu\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from typing import Tuple, List, Dict\n",
        "import requests\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# GPU Memory Management\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear CuPy memory pools to prevent OOM and mapping errors.\"\"\"\n",
        "    try:\n",
        "        np.get_default_memory_pool().free_all_blocks()\n",
        "        np.get_default_pinned_memory_pool().free_all_blocks()\n",
        "    except Exception as e:\n",
        "        print(f\"Memory cleanup warning: {e}\")\n",
        "\n",
        "# GPU â†” CPU Conversion\n",
        "def gpu_to_cpu(x):\n",
        "    \"\"\"CuPy GPU â†’ NumPy CPU (handles scalars + arrays).\"\"\"\n",
        "    if x is None:\n",
        "        return None\n",
        "    try:\n",
        "        if isinstance(x, (float, int, np_cpu.generic)):\n",
        "            return x\n",
        "        if hasattr(x, 'get'):\n",
        "            return x.get()\n",
        "        elif hasattr(x, 'item'):\n",
        "            return x.item()\n",
        "        else:\n",
        "            return np_cpu.asarray(x)\n",
        "    except Exception as e:\n",
        "        return np_cpu.asarray(x)\n",
        "\n",
        "def cpu_to_gpu(x):\n",
        "    \"\"\"NumPy CPU â†’ CuPy GPU.\"\"\"\n",
        "    if x is None: return None\n",
        "    # Avoid redundant copies if already on GPU\n",
        "    if hasattr(x, '__cuda_array_interface__'):\n",
        "        return x\n",
        "    return np.asarray(x)\n",
        "\n",
        "# Weighted RMSE Skill Score\n",
        "def weighted_rmse_score(y_true: np.ndarray, y_pred: np.ndarray, \n",
        "                        weights: np.ndarray) -> float:\n",
        "    \"\"\"GPU-accelerated metric. Returns Python float.\"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    weights = np.asarray(weights)\n",
        "    \n",
        "    score = 1 - np.sqrt(np.sum(weights * (y_true - y_pred) ** 2) / \n",
        "                        (np.sum(weights * y_true ** 2) + 1e-8))\n",
        "    return float(gpu_to_cpu(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Load Data & Baseline (Iter A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from data/train.parquet and data/test.parquet\n",
            "Warning: Train file not found. Creating dummy train.\n",
            "Warning: Test file not found. Creating dummy test.\n",
            "Loaded Train: 1 rows, Test: 1 rows\n",
            "Internal Validation split at ts_index >= 0\n",
            "Baseline (Mean Prediction) Score on Validation: nan\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Iteration A: Load Train & Test (Polars)\n",
        "\n",
        "def load_and_split_data(\n",
        "    train_path: str = \"data/train.parquet\",\n",
        "    test_path: str = \"data/test.parquet\",\n",
        "    target_col: str = \"feature_ch\",\n",
        "    weight_col: str = \"feature_cg\",\n",
        "    valid_ratio: float = 0.20,\n",
        ") -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame, list]:\n",
        "    print(f\"Loading data from {train_path} and {test_path}\")\n",
        "    \n",
        "    # Load Train\n",
        "    if os.path.exists(train_path):\n",
        "        train_full = pl.read_parquet(train_path)\n",
        "    else:\n",
        "        print(\"Warning: Train file not found. Creating dummy train.\")\n",
        "        train_full = pl.DataFrame({\"id\": [\"tr1\"], \"ts_index\": [0], \"horizon\": [1], target_col: [0.0], weight_col: [1.0]})\n",
        "        \n",
        "    # Load Test\n",
        "    if os.path.exists(test_path):\n",
        "        test_df = pl.read_parquet(test_path)\n",
        "    else:\n",
        "        print(\"Warning: Test file not found. Creating dummy test.\")\n",
        "        test_df = pl.DataFrame({\"id\": [\"te1\"], \"ts_index\": [100], \"horizon\": [1], target_col: [0.0], weight_col: [1.0]})\n",
        "\n",
        "    print(f\"Loaded Train: {train_full.height:,} rows, Test: {test_df.height:,} rows\")\n",
        "\n",
        "    # Time-based split on Train for internal validation\n",
        "    max_ts = train_full[\"ts_index\"].max()\n",
        "    min_ts = train_full[\"ts_index\"].min()\n",
        "    split_ts = max_ts - int((max_ts - min_ts) * valid_ratio)\n",
        "    \n",
        "    train_df = train_full.filter(pl.col(\"ts_index\") < split_ts)\n",
        "    valid_df = train_full.filter(pl.col(\"ts_index\") >= split_ts)\n",
        "\n",
        "    print(f\"Internal Validation split at ts_index >= {split_ts}\")\n",
        "    \n",
        "    # Feature columns (exclude meta)\n",
        "    exclude_cols = [\"id\", \"code\", \"sub_code\", \"sub_category\", target_col, weight_col, \"ts_index\", \"horizon\"]\n",
        "    feature_cols = [c for c in train_full.columns if c not in exclude_cols]\n",
        "\n",
        "    return train_df, valid_df, test_df, feature_cols\n",
        "\n",
        "# Execute Iter A\n",
        "train_df, valid_df, test_df, feature_cols = load_and_split_data()\n",
        "\n",
        "# Baseline Calculation on valid_df\n",
        "y_true_gpu = cpu_to_gpu(valid_df[\"feature_ch\"].to_numpy())\n",
        "weights_gpu = cpu_to_gpu(valid_df[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "\n",
        "train_mean_gpu = np.mean(cpu_to_gpu(train_df[\"feature_ch\"].to_numpy()))\n",
        "train_mean = float(train_mean_gpu)\n",
        "\n",
        "y_pred_baseline = np.ones_like(y_true_gpu) * train_mean\n",
        "baseline_score = weighted_rmse_score(y_true_gpu, y_pred_baseline, weights_gpu)\n",
        "print(f\"Baseline (Mean Prediction) Score on Validation: {baseline_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4: Temporal Features (Iter B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating advanced temporal features with Polars...\n"
          ]
        },
        {
          "ename": "ColumnNotFoundError",
          "evalue": "code",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mColumnNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3021765667.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Create advanced features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mfull_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_temporal_features_pl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Re-split accurately\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3021765667.py\u001b[0m in \u001b[0;36mcreate_temporal_features_pl\u001b[0;34m(df, feature_cols, group_cols, rolling_windows)\u001b[0m\n\u001b[1;32m      8\u001b[0m ) -> pl.DataFrame:\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating advanced temporal features with Polars...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"ts_index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mfeatures_to_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m30\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/dataframe/frame.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(self, by, descending, nulls_last, multithreaded, maintain_order, *more_by)\u001b[0m\n\u001b[1;32m   5710\u001b[0m                 \u001b[0mmaintain_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaintain_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5711\u001b[0m             )\n\u001b[0;32m-> 5712\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQueryOptFlags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5713\u001b[0m         )\n\u001b[1;32m   5714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/_utils/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"streaming\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/lazyframe/opt_flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizations\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptflags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/polars/lazyframe/frame.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         \u001b[0;31m# Only for testing purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m         \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post_opt_callback\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mColumnNotFoundError\u001b[0m: code"
          ]
        }
      ],
      "source": [
        "\n",
        "# Iteration B: Smart Temporal Feature Engineering (Polars)\n",
        "\n",
        "def create_temporal_features_pl(\n",
        "    df: pl.DataFrame,\n",
        "    feature_cols: List[str],\n",
        "    group_cols: List[str] = [\"code\", \"sub_code\", \"sub_category\"],\n",
        "    rolling_windows: List[int] = [3, 7, 14, 30],\n",
        ") -> pl.DataFrame:\n",
        "    print(\"Creating advanced temporal features with Polars...\")\n",
        "    df = df.sort(group_cols + [\"ts_index\"])\n",
        "    features_to_process = feature_cols[:30] if len(feature_cols) > 30 else feature_cols\n",
        "    \n",
        "    exprs = []\n",
        "    for feat in features_to_process:\n",
        "        for lag in [1, 2, 3, 5, 10]:\n",
        "            exprs.append(pl.col(feat).shift(lag).over(group_cols).alias(f\"{feat}_lag_{lag}\"))\n",
        "        exprs.append((pl.col(feat) - pl.col(feat).shift(1)).over(group_cols).alias(f\"{feat}_diff_1\"))\n",
        "        for window in rolling_windows:\n",
        "            shifted = pl.col(feat).shift(1)\n",
        "            exprs.append(shifted.rolling_mean(window_size=window, min_periods=1).over(group_cols).alias(f\"{feat}_roll_mean_{window}\"))\n",
        "            exprs.append(shifted.rolling_std(window_size=window, min_periods=1).over(group_cols).alias(f\"{feat}_roll_std_{window}\"))\n",
        "        shifted = pl.col(feat).shift(1)\n",
        "        exprs.append((shifted.cum_sum() / shifted.cum_count()).over(group_cols).alias(f\"{feat}_exp_mean\").fill_nan(0))\n",
        "\n",
        "    target = \"feature_ch\"\n",
        "    exprs.append(pl.col(target).shift(1).mean().over(group_cols).alias(\"group_target_avg\"))\n",
        "    exprs.append(pl.col(target).shift(1).std().over(group_cols).alias(\"group_target_vol\"))\n",
        "\n",
        "    df = df.with_columns(exprs)\n",
        "    return df\n",
        "\n",
        "# Combine with tracking\n",
        "train_df = train_df.with_columns(pl.lit(\"train\").alias(\"source_set\"))\n",
        "valid_df = valid_df.with_columns(pl.lit(\"valid\").alias(\"source_set\"))\n",
        "test_df = test_df.with_columns(pl.lit(\"test\").alias(\"source_set\"))\n",
        "\n",
        "full_df = pl.concat([train_df, valid_df, test_df], how=\"diagonal\") # Diagonal to handle missing target in test\n",
        "\n",
        "# Create advanced features\n",
        "full_df = create_temporal_features_pl(full_df, feature_cols)\n",
        "\n",
        "# Re-split accurately\n",
        "train_df = full_df.filter(pl.col(\"source_set\") == \"train\")\n",
        "valid_df = full_df.filter(pl.col(\"source_set\") == \"valid\")\n",
        "test_df = full_df.filter(pl.col(\"source_set\") == \"test\")\n",
        "\n",
        "current_features = [c for c in full_df.columns if c not in [\"id\", \"code\", \"sub_code\", \"sub_category\", \"feature_ch\", \"feature_cg\", \"ts_index\", \"horizon\", \"source_set\"]]\n",
        "print(f\"Total features after advanced Iter B: {len(current_features)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iteration C: Weighted LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteration C: Weighted LightGBM with Memory-Safe CV\n",
        "\n",
        "def train_lgb_model_cv(df, features, target=\"feature_ch\", weight=\"feature_cg\", n_folds=3):\n",
        "    \"\"\"Memory-efficient Time-Series CV for LightGBM.\"\"\"\n",
        "    clear_gpu_memory()\n",
        "    \n",
        "    # Sort for time-series split\n",
        "    df = df.sort(\"ts_index\")\n",
        "    ts_indices = df[\"ts_index\"].unique().sort()\n",
        "    \n",
        "    cv_scores = []\n",
        "    models = []\n",
        "    \n",
        "    # Simplified Expanding Window CV\n",
        "    for i in range(1, n_folds + 1):\n",
        "        split_idx = int(len(ts_indices) * (1 - 0.1 * i))\n",
        "        split_ts = ts_indices[split_idx]\n",
        "        \n",
        "        train_fold = df.filter(pl.col(\"ts_index\") < split_ts)\n",
        "        valid_fold = df.filter(pl.col(\"ts_index\") >= split_ts)\n",
        "        \n",
        "        if valid_fold.height == 0: continue\n",
        "        \n",
        "        print(f\"  Fold {i}: Train={train_fold.height}, Valid={valid_fold.height}\")\n",
        "        \n",
        "        # Data preparation (NO redundant GPU copies!)\n",
        "        # LightGBM can handle NumPy directly and will move to GPU if 'device': 'gpu'\n",
        "        X_tr = train_fold.select(features).fill_null(0).to_numpy()\n",
        "        y_tr = train_fold[target].to_numpy()\n",
        "        w_tr = train_fold[weight].fill_null(1.0).to_numpy()\n",
        "        \n",
        "        X_va = valid_fold.select(features).fill_null(0).to_numpy()\n",
        "        y_va = valid_fold[target].to_numpy()\n",
        "        w_va = valid_fold[weight].fill_null(1.0).to_numpy()\n",
        "        \n",
        "        train_data = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)\n",
        "        valid_data = lgb.Dataset(X_va, label=y_va, weight=w_va, reference=train_data)\n",
        "        \n",
        "        params = {\n",
        "            \"objective\": \"regression\",\n",
        "            \"metric\": \"rmse\",\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"num_leaves\": 31,\n",
        "            \"device\": \"gpu\",\n",
        "            \"verbose\": -1,\n",
        "            \"n_jobs\": -1\n",
        "        }\n",
        "        \n",
        "        model = lgb.train(\n",
        "            params, train_data, num_boost_round=500,\n",
        "            valid_sets=[valid_data],\n",
        "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(False)]\n",
        "        )\n",
        "        \n",
        "        preds = model.predict(X_va)\n",
        "        score = weighted_rmse_score(cpu_to_gpu(y_va), cpu_to_gpu(preds), cpu_to_gpu(w_va))\n",
        "        cv_scores.append(score)\n",
        "        models.append(model)\n",
        "        \n",
        "        # Explicitly delete large objects and clear pool\n",
        "        del X_tr, y_tr, w_tr, X_va, y_va, w_va, train_data, valid_data\n",
        "        clear_gpu_memory()\n",
        "        \n",
        "    avg_score = sum(cv_scores) / len(cv_scores) if cv_scores else 0\n",
        "    return models[-1], avg_score\n",
        "\n",
        "# Train separate models for horizons\n",
        "horizons = sorted(train_df[\"horizon\"].unique().to_list())\n",
        "best_models = {}\n",
        "\n",
        "print(\"Training Horizon-specific Models with CV (Iter C)...\")\n",
        "for h in horizons:\n",
        "    t_h = train_df.filter(pl.col(\"horizon\") == h)\n",
        "    if t_h.height == 0: continue\n",
        "        \n",
        "    print(f\"Processing Horizon {h}...\")\n",
        "    model, cv_score = train_lgb_model_cv(t_h, current_features)\n",
        "    best_models[h] = model\n",
        "    print(f\"âœ“ Horizon {h} CV Score: {cv_score:.4f}\")\n",
        "\n",
        "# Generate Predictions\n",
        "preds_full = []\n",
        "for h, model in best_models.items():\n",
        "    sub_df = valid_df.filter(pl.col(\"horizon\") == h)\n",
        "    if sub_df.height > 0:\n",
        "        preds = model.predict(sub_df.select(current_features).fill_null(0).to_numpy())\n",
        "        temp_df = sub_df.select(\"id\").with_columns(pl.Series(name=\"pred_iter_c_h\", values=preds))\n",
        "        preds_full.append(temp_df)\n",
        "\n",
        "if preds_full:\n",
        "    preds_all = pl.concat(preds_full)\n",
        "    valid_df = valid_df.join(preds_all, on=\"id\", how=\"left\").with_columns(\n",
        "        pl.col(\"pred_iter_c_h\").fill_null(0).alias(\"pred_iter_c\")\n",
        "    )\n",
        "\n",
        "# Overall evaluation\n",
        "y_true_gpu = cpu_to_gpu(valid_df[\"feature_ch\"].to_numpy())\n",
        "pred_gpu = cpu_to_gpu(valid_df[\"pred_iter_c\"].to_numpy())\n",
        "weights_gpu = cpu_to_gpu(valid_df[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "overall_score_c = weighted_rmse_score(y_true_gpu, pred_gpu, weights_gpu)\n",
        "print(f\"Overall Iteration C Score: {overall_score_c:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iteration D: PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Iteration D: PCA (GPU-optimized & Memory-Safe)\n",
        "\n",
        "print(\"Applying PCA (Iter D)...\")\n",
        "clear_gpu_memory()\n",
        "\n",
        "# Select numeric features\n",
        "pca_features = [c for c in train_df.columns if c.startswith(\"feature_\") or \"_roll_\" in c]\n",
        "pca_features = pca_features[:50]\n",
        "\n",
        "# Load to GPU\n",
        "X_train_gpu = cpu_to_gpu(train_df.select(pca_features).fill_null(0).to_numpy())\n",
        "X_valid_gpu = cpu_to_gpu(valid_df.select(pca_features).fill_null(0).to_numpy())\n",
        "X_test_gpu = cpu_to_gpu(test_df.select(pca_features).fill_null(0).to_numpy())\n",
        "\n",
        "# Standardize\n",
        "mean_gpu = np.mean(X_train_gpu, axis=0, keepdims=True)\n",
        "std_gpu = np.std(X_train_gpu, axis=0, keepdims=True)\n",
        "std_gpu = np.where(std_gpu == 0, 1.0, std_gpu)\n",
        "\n",
        "X_train_scaled = (X_train_gpu - mean_gpu) / std_gpu\n",
        "X_valid_scaled = (X_valid_gpu - mean_gpu) / std_gpu\n",
        "X_test_scaled = (X_test_gpu - mean_gpu) / std_gpu\n",
        "\n",
        "# PCA on CPU\n",
        "pca = PCA(n_components=10)\n",
        "X_train_pca = pca.fit_transform(gpu_to_cpu(X_train_scaled))\n",
        "X_valid_pca = pca.transform(gpu_to_cpu(X_valid_scaled))\n",
        "X_test_pca = pca.transform(gpu_to_cpu(X_test_scaled))\n",
        "\n",
        "# Add back\n",
        "pca_cols = [f\"pca_{i}\" for i in range(10)]\n",
        "train_df = pl.concat([train_df, pl.DataFrame(X_train_pca, schema=pca_cols)], how=\"horizontal\")\n",
        "valid_df = pl.concat([valid_df, pl.DataFrame(X_valid_pca, schema=pca_cols)], how=\"horizontal\")\n",
        "test_df = pl.concat([test_df, pl.DataFrame(X_test_pca, schema=pca_cols)], how=\"horizontal\")\n",
        "\n",
        "features_d = current_features + pca_cols\n",
        "print(f\"PCA features added to all sets. Total: {len(features_d)}\")\n",
        "del X_train_gpu, X_valid_gpu, X_test_gpu, X_train_scaled, X_valid_scaled, X_test_scaled\n",
        "clear_gpu_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iteration E: Smoothed Target Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteration E: Smoothed Target Encoding (Polars Fast)\n",
        "\n",
        "def create_smoothed_target_encoding_pl(\n",
        "    df, col, target=\"feature_ch\", weight=\"feature_cg\", min_samples=10, smoothing=10\n",
        "):\n",
        "    df = df.sort([col, \"ts_index\"])\n",
        "    global_mean = df[target].mean()\n",
        "    \n",
        "    return df.with_columns(\n",
        "        (\n",
        "            (pl.col(target).shift(1).cum_sum().over(col).fill_null(0) + smoothing * global_mean) \n",
        "            / \n",
        "            (pl.col(target).shift(1).cum_count().over(col).fill_null(0) + smoothing)\n",
        "        ).alias(f\"{col}_enc_smooth\")\n",
        "    )\n",
        "\n",
        "print(\"Applying Smoothed Target Encoding (Iter E)...\")\n",
        "cols_in_common = [c for c in train_df.columns if c in valid_df.columns and c in test_df.columns]\n",
        "full_df = pl.concat([\n",
        "    train_df.select(cols_in_common), \n",
        "    valid_df.select(cols_in_common), \n",
        "    test_df.select(cols_in_common)\n",
        "], how=\"diagonal\")\n",
        "\n",
        "for col in [\"code\", \"sub_code\", \"sub_category\"]:\n",
        "    full_df = create_smoothed_target_encoding_pl(full_df, col)\n",
        "\n",
        "# Re-split\n",
        "train_df = full_df.filter(pl.col(\"source_set\") == \"train\")\n",
        "valid_df = full_df.filter(pl.col(\"source_set\") == \"valid\")\n",
        "test_df = full_df.filter(pl.col(\"source_set\") == \"test\")\n",
        "\n",
        "features_e = features_d + [f\"{c}_enc_smooth\" for c in [\"code\", \"sub_code\", \"sub_category\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 8: Feature Selection (Iter F)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteration F: Advanced Feature Selection (Model-Based)\n",
        "\n",
        "print(\"Applying Non-Linear Feature Selection (Iter F)...\")\n",
        "\n",
        "# 1. Create Interactions\n",
        "new_cols = []\n",
        "top_feats = [c for c in features_e if \"_lag_1\" in c or \"_roll_mean_7\" in c][:10]\n",
        "for feat in top_feats:\n",
        "    new_cols.append((pl.col(feat) * pl.col(\"horizon\")).alias(f\"{feat}_x_horizon\"))\n",
        "\n",
        "train_df = train_df.with_columns(new_cols)\n",
        "valid_df = valid_df.with_columns(new_cols)\n",
        "test_df = test_df.with_columns(new_cols)\n",
        "\n",
        "all_candidates_f = features_e + [f\"{feat}_x_horizon\" for feat in top_feats]\n",
        "all_candidates_f = [c for c in all_candidates_f if c in train_df.columns]\n",
        "\n",
        "# 2. Use LightGBM Importance for Selection\n",
        "X_sel_np = train_df.select(all_candidates_f).fill_null(0).to_numpy()\n",
        "y_sel_np = train_df[\"feature_ch\"].to_numpy()\n",
        "\n",
        "sel_model = lgb.LGBMRegressor(n_estimators=100, device=\"gpu\", random_state=42, verbose=-1)\n",
        "sel_model.fit(X_sel_np, y_sel_np)\n",
        "\n",
        "importance_df = pl.DataFrame({\n",
        "    \"feature\": all_candidates_f,\n",
        "    \"importance\": sel_model.feature_importances_\n",
        "}).sort(\"importance\", descending=True)\n",
        "\n",
        "selected_features_f = importance_df.head(150)[\"feature\"].to_list()\n",
        "print(f\"âœ“ Selected top {len(selected_features_f)} features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iteration G: Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iteration G: The \"Silver Bullet\" Ensemble (Final Submission on test.parquet)\n",
        "\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "print(\"Training Integrated GPU Ensemble & Generating Final Submission...\")\n",
        "clear_gpu_memory()\n",
        "\n",
        "horizons = sorted(train_df[\"horizon\"].unique().to_list())\n",
        "preds_test = []\n",
        "preds_valid = []\n",
        "\n",
        "for h in horizons:\n",
        "    t_h = train_df.filter(pl.col(\"horizon\") == h)\n",
        "    v_h = valid_df.filter(pl.col(\"horizon\") == h)\n",
        "    te_h = test_df.filter(pl.col(\"horizon\") == h)\n",
        "    \n",
        "    if t_h.height == 0: continue\n",
        "    \n",
        "    print(f\"Processing Horizon {h}...\")\n",
        "    \n",
        "    X_train = t_h.select(selected_features_f).fill_null(0).to_numpy()\n",
        "    y_train = t_h[\"feature_ch\"].to_numpy()\n",
        "    w_train = t_h[\"feature_cg\"].fill_null(1.0).to_numpy()\n",
        "    \n",
        "    X_valid = v_h.select(selected_features_f).fill_null(0).to_numpy()\n",
        "    X_test = te_h.select(selected_features_f).fill_null(0).to_numpy()\n",
        "    \n",
        "    # Simple weighted blend\n",
        "    def train_and_predict(X_tr, y_tr, w_tr, X_val, X_te):\n",
        "        # LGBM\n",
        "        m1 = lgb.LGBMRegressor(n_estimators=500, device=\"gpu\", random_state=42, verbose=-1)\n",
        "        m1.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "        p1_v, p1_t = m1.predict(X_val), m1.predict(X_te)\n",
        "        \n",
        "        # XGB\n",
        "        m2 = xgb.XGBRegressor(n_estimators=500, tree_method=\"hist\", device=\"cuda\", random_state=42)\n",
        "        m2.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "        p2_v, p2_t = m2.predict(X_val), m2.predict(X_te)\n",
        "        \n",
        "        # CatBoost\n",
        "        m3 = CatBoostRegressor(n_estimators=500, task_type=\"GPU\", random_state=42, verbose=0)\n",
        "        m3.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "        p3_v, p3_t = m3.predict(X_val), m3.predict(X_te)\n",
        "        \n",
        "        clear_gpu_memory()\n",
        "        v_res = (0.4 * p1_v + 0.4 * p2_v + 0.2 * p3_v)\n",
        "        t_res = (0.4 * p1_t + 0.4 * p2_t + 0.2 * p3_t)\n",
        "        return v_res, t_res\n",
        "\n",
        "    p_val, p_test = train_and_predict(X_train, y_train, w_train, X_valid, X_test)\n",
        "    \n",
        "    preds_valid.append(v_h.select(\"id\").with_columns(pl.Series(\"prediction\", p_val)))\n",
        "    preds_test.append(te_h.select(\"id\").with_columns(pl.Series(\"prediction\", p_test)))\n",
        "\n",
        "# Save Submission\n",
        "if preds_test:\n",
        "    sub = pl.concat(preds_test)\n",
        "    sub.write_csv(\"submission_final_polars.csv\")\n",
        "    print(f\"âœ“ Saved submission with {sub.height} rows\")\n",
        "\n",
        "# Validation Score\n",
        "if preds_valid:\n",
        "    val_res = pl.concat(preds_valid)\n",
        "    val_merged = valid_df.join(val_res, on=\"id\")\n",
        "    y_true = cpu_to_gpu(val_merged[\"feature_ch\"].to_numpy())\n",
        "    y_pred = cpu_to_gpu(val_merged[\"prediction\"].to_numpy())\n",
        "    weights = cpu_to_gpu(val_merged[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "    score = weighted_rmse_score(y_true, y_pred, weights)\n",
        "    print(f\"ðŸš€ Final Validation Skill Score: {score:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
