{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-1-8df04e2c",
        "language": "markdown"
      },
      "source": [
        "# Step 0: Download Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-2-d6079237",
        "language": "markdown"
      },
      "source": [
        "\n",
        "# GPU-Accelerated Time Series Forecasting\n",
        "\n",
        "**Hardware**: RTX 3070 + CUDA 13.1 + CuPy 13.3.0 + LightGBM\n",
        "\n",
        "**7-Step Pipeline**:\n",
        "1. Load data & baseline metric\n",
        "2. Temporal features (lags, rolling windows)\n",
        "3. Horizon-specific LightGBM models\n",
        "4. PCA dimensionality reduction\n",
        "5. Smoothed target encoding\n",
        "6. Feature selection & interactions\n",
        "7. Ensemble predictions\n",
        "\n",
        "**Key Design**: Matrix operations (GPU) â†’ CuPy. External libs (CPU) â†’ NumPy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ce2974b",
      "metadata": {
        "id": "cell-3-ebd29ce5",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# GPU Setup & Initialization\n",
        "\n",
        "import cupy as np\n",
        "import numpy as np_cpu\n",
        "import sys\n",
        "\n",
        "print(f\"Python: {sys.executable}\")\n",
        "print(f\"CuPy Version: {np.__version__}\")\n",
        "\n",
        "# Check GPU\n",
        "try:\n",
        "    device_count = np.cuda.runtime.getDeviceCount()\n",
        "    device = np.cuda.Device(0)\n",
        "    cap = device.compute_capability\n",
        "    print(f\"âœ“ GPU Ready: {device_count} device(s), Compute Capability {cap}\")\n",
        "except Exception as e:\n",
        "    print(f\"âœ— GPU Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "814aeccc",
      "metadata": {
        "id": "cell-4-783c87cd",
        "language": "markdown"
      },
      "source": [
        "\n",
        "# Step 1: Download Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e0aba51",
      "metadata": {
        "id": "cell-5-6ed904ab",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Kaggle Data Download (Cross-Platform)\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import zipfile\n",
        "import platform\n",
        "\n",
        "def download_kaggle_data(competition_name=\"ts-forecasting\"):\n",
        "    data_dir = \"data\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "    \n",
        "    # Check if files already exist\n",
        "    train_exists = os.path.exists(os.path.join(data_dir, \"train.parquet\"))\n",
        "    test_exists = os.path.exists(os.path.join(data_dir, \"test.parquet\"))\n",
        "    \n",
        "    if train_exists and test_exists:\n",
        "        print(\"âœ“ Data files already exist in 'data/'. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(f\"[{platform.system()}] Downloading data for competition '{competition_name}'...\")\n",
        "    \n",
        "    # Configure Credentials\n",
        "    env = os.environ.copy()\n",
        "    token = \"KGAT_ccc00b322d3c4b85f0036a23cc420469\"\n",
        "    env[\"KAGGLE_USERNAME\"] = \"dummy_user\"\n",
        "    env[\"KAGGLE_KEY\"] = token\n",
        "    env[\"KAGGLE_API_TOKEN\"] = token\n",
        "\n",
        "    # Detect Kaggle Executable Path\n",
        "    kaggle_cmd = \"kaggle\"\n",
        "    try:\n",
        "        # Check if 'kaggle' is in PATH\n",
        "        subprocess.run([kaggle_cmd, \"--version\"], capture_output=True, check=True)\n",
        "    except:\n",
        "        # If not, try common venv locations\n",
        "        if platform.system() == \"Windows\":\n",
        "            venv_kaggle = os.path.join(\".venv\", \"Scripts\", \"kaggle.exe\")\n",
        "        else:\n",
        "            venv_kaggle = os.path.join(\".venv\", \"bin\", \"kaggle\")\n",
        "            \n",
        "        if os.path.exists(venv_kaggle):\n",
        "            kaggle_cmd = venv_kaggle\n",
        "        else:\n",
        "            print(f\"! Warning: 'kaggle' CLI not found in PATH or .venv. Attempting default 'kaggle' anyway.\")\n",
        "\n",
        "    try:\n",
        "        # 1. Download\n",
        "        result = subprocess.run([\n",
        "            kaggle_cmd, \"competitions\", \"download\", \"-c\", competition_name\n",
        "        ], check=True, env=env, capture_output=True, text=True)\n",
        "        \n",
        "        # 2. Extract\n",
        "        zip_path = f\"{competition_name}.zip\"\n",
        "        if os.path.exists(zip_path):\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(data_dir)\n",
        "            os.remove(zip_path)\n",
        "            print(f\"âœ“ Downloaded and extracted to {data_dir}/\")\n",
        "        else:\n",
        "            # Check for alternative zip names\n",
        "            zips = [f for f in os.listdir('.') if f.endswith('.zip')]\n",
        "            if zips:\n",
        "                with zipfile.ZipFile(zips[0], 'r') as zip_ref:\n",
        "                    zip_ref.extractall(data_dir)\n",
        "                os.remove(zips[0])\n",
        "                print(f\"âœ“ Extracted {zips[0]} to {data_dir}/\")\n",
        "            else:\n",
        "                print(\"âœ— Zip file not found after download.\")\n",
        "                \n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"âœ— Failed to download via Kaggle API.\")\n",
        "        print(f\"Error: {e.stderr if e.stderr else e.stdout}\")\n",
        "        print(\"Tip: If error is 403, ensure you accepted competition rules on Kaggle website.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— General Error: {e}\")\n",
        "\n",
        "download_kaggle_data()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002a08a9",
      "metadata": {
        "id": "cell-6-400274e1",
        "language": "markdown"
      },
      "source": [
        "\n",
        "# Step 2: Imports & Metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aba94c5",
      "metadata": {
        "id": "cell-7-1e72ad7d",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Utilities & Metric\n",
        "\n",
        "import polars as pl\n",
        "import warnings\n",
        "import os\n",
        "import lightgbm as lgb\n",
        "import cupy as np\n",
        "import numpy as np_cpu\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from typing import Tuple, List, Dict\n",
        "import requests\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# GPU Memory Management\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear CuPy memory pools to prevent OOM and mapping errors.\"\"\"\n",
        "    try:\n",
        "        np.get_default_memory_pool().free_all_blocks()\n",
        "        np.get_default_pinned_memory_pool().free_all_blocks()\n",
        "    except Exception as e:\n",
        "        print(f\"Memory cleanup warning: {e}\")\n",
        "\n",
        "# GPU â†” CPU Conversion\n",
        "def gpu_to_cpu(x):\n",
        "    \"\"\"CuPy GPU â†’ NumPy CPU (handles scalars + arrays).\"\"\"\n",
        "    if x is None:\n",
        "        return None\n",
        "    try:\n",
        "        if isinstance(x, (float, int, np_cpu.generic)):\n",
        "            return x\n",
        "        if hasattr(x, 'get'):\n",
        "            return x.get()\n",
        "        elif hasattr(x, 'item'):\n",
        "            return x.item()\n",
        "        else:\n",
        "            return np_cpu.asarray(x)\n",
        "    except Exception as e:\n",
        "        return np_cpu.asarray(x)\n",
        "\n",
        "def cpu_to_gpu(x):\n",
        "    \"\"\"NumPy CPU â†’ CuPy GPU.\"\"\"\n",
        "    if x is None: return None\n",
        "    # Avoid redundant copies if already on GPU\n",
        "    if hasattr(x, '__cuda_array_interface__'):\n",
        "        return x\n",
        "    return np.asarray(x)\n",
        "\n",
        "# Weighted RMSE Skill Score\n",
        "def weighted_rmse_score(y_true: np.ndarray, y_pred: np.ndarray, \n",
        "                        weights: np.ndarray) -> float:\n",
        "    \"\"\"GPU-accelerated metric. Returns Python float.\"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    weights = np.asarray(weights)\n",
        "    \n",
        "    score = 1 - np.sqrt(np.sum(weights * (y_true - y_pred) ** 2) / \n",
        "                        (np.sum(weights * y_true ** 2) + 1e-8))\n",
        "    return float(gpu_to_cpu(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad8ce02d",
      "metadata": {
        "id": "cell-8-cff11c58",
        "language": "markdown"
      },
      "source": [
        "# Step 3: Load Data & Baseline (Iter A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7cd2ea0",
      "metadata": {
        "id": "cell-9-74ab4d34",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Iteration A: Load Train & Test (Polars) - Ultra-Lean Memory Strategy\n",
        "import gc\n",
        "from typing import Tuple, List\n",
        "\n",
        "def load_and_split_data(\n",
        "    train_path: str = \"data/train.parquet\",\n",
        "    test_path: str = \"data/test.parquet\",\n",
        "    target_col: str = \"feature_ch\",\n",
        "    weight_col: str = \"feature_cg\",\n",
        "    valid_ratio: float = 0.20,\n",
        "    ) -> Tuple[pl.DataFrame, pl.DataFrame, pl.DataFrame, list]:\n",
        "    print(f\"Loading data from {train_path} and {test_path} (Ultra-Lean)...\")\n",
        "    \n",
        "    def optimize_df(df):\n",
        "        # Cast floats to 32-bit\n",
        "        float_cols = [c for c, t in df.schema.items() if t == pl.Float64]\n",
        "        # Cast strings/objects to Categorical for 10x memory saving\n",
        "        str_cols = [c for c, t in df.schema.items() if t == pl.Utf8 or t == pl.String]\n",
        "        \n",
        "        ops = []\n",
        "        if float_cols: ops.extend([pl.col(c).cast(pl.Float32) for c in float_cols])\n",
        "        if str_cols: ops.extend([pl.col(c).cast(pl.Categorical) for c in str_cols])\n",
        "        \n",
        "        return df.with_columns(ops) if ops else df\n",
        "\n",
        "    # Load Train\n",
        "    if os.path.exists(train_path):\n",
        "        train_full = optimize_df(pl.read_parquet(train_path))\n",
        "    else:\n",
        "        print(\"Warning: Train file not found. Creating dummy train.\")\n",
        "        train_full = pl.DataFrame({\"id\": [\"tr1\"], \"ts_index\": [0], \"horizon\": [1], target_col: [0.0], weight_col: [1.0]})\n",
        "        train_full = optimize_df(train_full)\n",
        "        \n",
        "    # Load Test\n",
        "    if os.path.exists(test_path):\n",
        "        test_df = optimize_df(pl.read_parquet(test_path))\n",
        "    else:\n",
        "        print(\"Warning: Test file not found. Creating dummy test.\")\n",
        "        test_df = pl.DataFrame({\"id\": [\"te1\"], \"ts_index\": [100], \"horizon\": [1], target_col: [0.0], weight_col: [1.0]})\n",
        "        test_df = optimize_df(test_df)\n",
        "\n",
        "    print(f\"Loaded Train: {train_full.height:,} rows, Test: {test_df.height:,} rows (Floats -> 32bit, Strings -> Categorical)\")\n",
        "\n",
        "    # Time-based split\n",
        "    max_ts = train_full[\"ts_index\"].max()\n",
        "    min_ts = train_full[\"ts_index\"].min()\n",
        "    split_ts = max_ts - int((max_ts - min_ts) * valid_ratio)\n",
        "    \n",
        "    train_df = train_full.filter(pl.col(\"ts_index\") < split_ts)\n",
        "    valid_df = train_full.filter(pl.col(\"ts_index\") >= split_ts)\n",
        "    \n",
        "    del train_full\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Internal Validation split at ts_index >= {split_ts}\")\n",
        "    \n",
        "    exclude_cols = [\"id\", \"code\", \"sub_code\", \"sub_category\", target_col, weight_col, \"ts_index\", \"horizon\"]\n",
        "    exclude_cols = [c for c in exclude_cols if c in train_df.columns]\n",
        "    feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
        "\n",
        "    return train_df, valid_df, test_df, feature_cols\n",
        "\n",
        "# Execute Iter A\n",
        "train_df, valid_df, test_df, feature_cols = load_and_split_data()\n",
        "\n",
        "# Baseline Prediction (GPU)\n",
        "y_true_gpu = cpu_to_gpu(valid_df[\"feature_ch\"].to_numpy())\n",
        "weights_gpu = cpu_to_gpu(valid_df[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "train_mean = float(np.mean(cpu_to_gpu(train_df[\"feature_ch\"].to_numpy())))\n",
        "\n",
        "y_pred_baseline = np.ones_like(y_true_gpu) * train_mean\n",
        "baseline_score = weighted_rmse_score(y_true_gpu, y_pred_baseline, weights_gpu)\n",
        "print(f\"Baseline (Mean Prediction) Score on Validation: {baseline_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d2e90a",
      "metadata": {
        "id": "cell-10-fac6ca26",
        "language": "markdown"
      },
      "source": [
        "# Step 4: Temporal Features (Iter B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477178fc",
      "metadata": {
        "id": "cell-11-c69eccda",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Iteration B: Smart Temporal Feature Engineering (Polars) \n",
        "def create_temporal_features_pl(\n",
        "    df: pl.DataFrame,\n",
        "    feature_cols: List[str],\n",
        "    group_cols: List[str] = [\"code\", \"sub_code\"],\n",
        "    rolling_windows: List[int] = [7, 30],\n",
        ") -> pl.DataFrame:\n",
        "    # Use top 15 features for absolute RAM safety on 16GB systems\n",
        "    features_to_process = feature_cols[:15]\n",
        "    print(f\"Creating features for {len(features_to_process)} features...\")\n",
        "    \n",
        "    # Sort\n",
        "    group_cols_existing = [c for c in group_cols if c in df.columns]\n",
        "    df = df.sort(group_cols_existing + [\"ts_index\"])\n",
        "    \n",
        "    batch_size = 5\n",
        "    for i in range(0, len(features_to_process), batch_size):\n",
        "        batch = features_to_process[i:i+batch_size]\n",
        "        print(f\"  Batch {i//batch_size + 1}: {batch}\")\n",
        "        \n",
        "        exprs = []\n",
        "        for feat in batch:\n",
        "            exprs.append(pl.col(feat).shift(1).over(group_cols_existing).alias(f\"{feat}_lag1\").cast(pl.Float32))\n",
        "            exprs.append(pl.col(feat).shift(2).over(group_cols_existing).alias(f\"{feat}_lag2\").cast(pl.Float32))\n",
        "            for w in rolling_windows:\n",
        "                exprs.append(pl.col(feat).shift(1).rolling_mean(w, min_periods=1).over(group_cols_existing).alias(f\"{feat}_rm{w}\").cast(pl.Float32))\n",
        "        \n",
        "        df = df.with_columns(exprs)\n",
        "        gc.collect()\n",
        "\n",
        "    return df\n",
        "\n",
        "# Start Fresh\n",
        "gc.collect()\n",
        "train_df = train_df.with_columns(pl.lit(\"train\").alias(\"source_set\").cast(pl.Categorical))\n",
        "valid_df = valid_df.with_columns(pl.lit(\"valid\").alias(\"source_set\").cast(pl.Categorical))\n",
        "test_df = test_df.with_columns(pl.lit(\"test\").alias(\"source_set\").cast(pl.Categorical))\n",
        "\n",
        "full_df = pl.concat([train_df, valid_df, test_df], how=\"diagonal\")\n",
        "del train_df, valid_df, test_df\n",
        "gc.collect()\n",
        "\n",
        "full_df = create_temporal_features_pl(full_df, feature_cols)\n",
        "\n",
        "print(\"Splitting Destructively...\")\n",
        "train_df = full_df.filter(pl.col(\"source_set\") == \"train\")\n",
        "full_df = full_df.filter(pl.col(\"source_set\") != \"train\")\n",
        "gc.collect()\n",
        "\n",
        "valid_df = full_df.filter(pl.col(\"source_set\") == \"valid\")\n",
        "full_df = full_df.filter(pl.col(\"source_set\") != \"valid\")\n",
        "gc.collect()\n",
        "\n",
        "test_df = full_df.filter(pl.col(\"source_set\") == \"test\")\n",
        "del full_df\n",
        "gc.collect()\n",
        "\n",
        "current_features = [c for c in train_df.columns if c not in [\"id\", \"code\", \"sub_code\", \"sub_category\", \"feature_ch\", \"feature_cg\", \"ts_index\", \"horizon\", \"source_set\"]]\n",
        "print(f\"Total features after Iter B: {len(current_features)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca5f24f",
      "metadata": {
        "id": "cell-12-ab6508f7",
        "language": "markdown"
      },
      "source": [
        "# Iteration C: Weighted LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1836073",
      "metadata": {
        "id": "cell-13-cae6d238",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Iteration C: Weighted LightGBM with Memory-Safe CV\n",
        "\n",
        "def train_lgb_model_cv(df, features, target=\"feature_ch\", weight=\"feature_cg\", n_folds=3):\n",
        "    \"\"\"Memory-efficient Time-Series CV for LightGBM.\"\"\"\n",
        "    clear_gpu_memory()\n",
        "    \n",
        "    # Sort for time-series split\n",
        "    df = df.sort(\"ts_index\")\n",
        "    ts_indices = df[\"ts_index\"].unique().sort()\n",
        "    \n",
        "    cv_scores = []\n",
        "    models = []\n",
        "    \n",
        "    # Simplified Expanding Window CV\n",
        "    for i in range(1, n_folds + 1):\n",
        "        split_idx = int(len(ts_indices) * (1 - 0.1 * i))\n",
        "        split_ts = ts_indices[split_idx]\n",
        "        \n",
        "        train_fold = df.filter(pl.col(\"ts_index\") < split_ts)\n",
        "        valid_fold = df.filter(pl.col(\"ts_index\") >= split_ts)\n",
        "        \n",
        "        if valid_fold.height == 0: continue\n",
        "        \n",
        "        print(f\"  Fold {i}: Train={train_fold.height}, Valid={valid_fold.height}\")\n",
        "        \n",
        "        # Data preparation (NO redundant GPU copies!)\n",
        "        # LightGBM can handle NumPy directly and will move to GPU if 'device': 'gpu'\n",
        "        X_tr = train_fold.select(features).fill_null(0).to_numpy()\n",
        "        y_tr = train_fold[target].to_numpy()\n",
        "        w_tr = train_fold[weight].fill_null(1.0).to_numpy()\n",
        "        \n",
        "        X_va = valid_fold.select(features).fill_null(0).to_numpy()\n",
        "        y_va = valid_fold[target].to_numpy()\n",
        "        w_va = valid_fold[weight].fill_null(1.0).to_numpy()\n",
        "        \n",
        "        train_data = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)\n",
        "        valid_data = lgb.Dataset(X_va, label=y_va, weight=w_va, reference=train_data)\n",
        "        \n",
        "        params = {\n",
        "            \"objective\": \"regression\",\n",
        "            \"metric\": \"rmse\",\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"num_leaves\": 31,\n",
        "            \"device\": \"gpu\",\n",
        "            \"verbose\": -1,\n",
        "            \"n_jobs\": -1\n",
        "        }\n",
        "        \n",
        "        model = lgb.train(\n",
        "            params, train_data, num_boost_round=500,\n",
        "            valid_sets=[valid_data],\n",
        "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(False)]\n",
        "        )\n",
        "        \n",
        "        preds = model.predict(X_va)\n",
        "        score = weighted_rmse_score(cpu_to_gpu(y_va), cpu_to_gpu(preds), cpu_to_gpu(w_va))\n",
        "        cv_scores.append(score)\n",
        "        models.append(model)\n",
        "        \n",
        "        # Explicitly delete large objects and clear pool\n",
        "        del X_tr, y_tr, w_tr, X_va, y_va, w_va, train_data, valid_data\n",
        "        clear_gpu_memory()\n",
        "        \n",
        "    avg_score = sum(cv_scores) / len(cv_scores) if cv_scores else 0\n",
        "    return models[-1], avg_score\n",
        "\n",
        "# Train separate models for horizons\n",
        "horizons = sorted(train_df[\"horizon\"].unique().to_list())\n",
        "best_models = {}\n",
        "\n",
        "print(\"Training Horizon-specific Models with CV (Iter C)...\")\n",
        "for h in horizons:\n",
        "    t_h = train_df.filter(pl.col(\"horizon\") == h)\n",
        "    if t_h.height == 0: continue\n",
        "        \n",
        "    print(f\"Processing Horizon {h}...\")\n",
        "    model, cv_score = train_lgb_model_cv(t_h, current_features)\n",
        "    best_models[h] = model\n",
        "    print(f\"âœ“ Horizon {h} CV Score: {cv_score:.4f}\")\n",
        "\n",
        "# Generate Predictions\n",
        "preds_full = []\n",
        "for h, model in best_models.items():\n",
        "    sub_df = valid_df.filter(pl.col(\"horizon\") == h)\n",
        "    if sub_df.height > 0:\n",
        "        preds = model.predict(sub_df.select(current_features).fill_null(0).to_numpy())\n",
        "        temp_df = sub_df.select(\"id\").with_columns(pl.Series(name=\"pred_iter_c_h\", values=preds))\n",
        "        preds_full.append(temp_df)\n",
        "\n",
        "if preds_full:\n",
        "    preds_all = pl.concat(preds_full)\n",
        "    valid_df = valid_df.join(preds_all, on=\"id\", how=\"left\").with_columns(\n",
        "        pl.col(\"pred_iter_c_h\").fill_null(0).alias(\"pred_iter_c\")\n",
        "    )\n",
        "\n",
        "# Overall evaluation\n",
        "y_true_gpu = cpu_to_gpu(valid_df[\"feature_ch\"].to_numpy())\n",
        "pred_gpu = cpu_to_gpu(valid_df[\"pred_iter_c\"].to_numpy())\n",
        "weights_gpu = cpu_to_gpu(valid_df[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "overall_score_c = weighted_rmse_score(y_true_gpu, pred_gpu, weights_gpu)\n",
        "print(f\"Overall Iteration C Score: {overall_score_c:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca73afdf",
      "metadata": {
        "id": "cell-14-a930987a",
        "language": "markdown"
      },
      "source": [
        "# Iteration D: PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437c221b",
      "metadata": {
        "id": "cell-15-92b9313d",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Iteration D: PCA (GPU-optimized & Memory-Safe)\n",
        "\n",
        "print(\"Applying PCA (Iter D)...\")\n",
        "clear_gpu_memory()\n",
        "\n",
        "# Select numeric features (Updated for new names: _rm, _lag)\n",
        "pca_features = [c for c in train_df.columns if c.startswith(\"feature_\") or \"_rm\" in c or \"_lag\" in c]\n",
        "pca_features = pca_features[:40] # Reduced to 40 for stability\n",
        "\n",
        "print(f\"  Using {len(pca_features)} features for PCA\")\n",
        "\n",
        "# Load to GPU (Batch process if needed, but 40 cols should fit)\n",
        "def get_scaled_gpu(df, cols, mean=None, std=None):\n",
        "    X = cpu_to_gpu(df.select(cols).fill_null(0).to_numpy())\n",
        "    if mean is None:\n",
        "        mean = np.mean(X, axis=0, keepdims=True)\n",
        "        std = np.std(X, axis=0, keepdims=True)\n",
        "        std = np.where(std == 0, 1.0, std)\n",
        "    X = (X - mean) / std\n",
        "    return X, mean, std\n",
        "\n",
        "X_train_scaled, m, s = get_scaled_gpu(train_df, pca_features)\n",
        "\n",
        "# PCA on CPU (sklearn)\n",
        "pca = PCA(n_components=8) # Reduced components for RAM\n",
        "X_train_pca = pca.fit_transform(gpu_to_cpu(X_train_scaled))\n",
        "del X_train_scaled\n",
        "gc.collect()\n",
        "\n",
        "# Validation\n",
        "X_valid_scaled, _, _ = get_scaled_gpu(valid_df, pca_features, m, s)\n",
        "X_valid_pca = pca.transform(gpu_to_cpu(X_valid_scaled))\n",
        "del X_valid_scaled\n",
        "gc.collect()\n",
        "\n",
        "# Test\n",
        "X_test_scaled, _, _ = get_scaled_gpu(test_df, pca_features, m, s)\n",
        "X_test_pca = pca.transform(gpu_to_cpu(X_test_scaled))\n",
        "del X_test_scaled\n",
        "clear_gpu_memory()\n",
        "\n",
        "# Add back as Float32\n",
        "pca_cols = [f\"pca_{i}\" for i in range(8)]\n",
        "train_df = pl.concat([train_df, pl.DataFrame(X_train_pca, schema=pca_cols).with_columns([pl.all().cast(pl.Float32)])], how=\"horizontal\")\n",
        "valid_df = pl.concat([valid_df, pl.DataFrame(X_valid_pca, schema=pca_cols).with_columns([pl.all().cast(pl.Float32)])], how=\"horizontal\")\n",
        "test_df = pl.concat([test_df, pl.DataFrame(X_test_pca, schema=pca_cols).with_columns([pl.all().cast(pl.Float32)])], how=\"horizontal\")\n",
        "\n",
        "features_d = current_features + pca_cols\n",
        "print(f\"âœ“ PCA features added. Total candidates: {len(features_d)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5fad499",
      "metadata": {
        "id": "cell-16-0035b3f9",
        "language": "markdown"
      },
      "source": [
        "# Iteration E: Smoothed Target Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff882f83",
      "metadata": {
        "id": "cell-17-f0ac8086",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Iteration E: Smoothed Target Encoding (RAM-Safe Hstack)\n",
        "\n",
        "def create_smoothed_target_encoding_pl(\n",
        "    df, col, target=\"feature_ch\", smoothing=10\n",
        "):\n",
        "    if col not in df.columns: return df\n",
        "    global_mean = df[target].mean()\n",
        "    \n",
        "    # Use shift(1) to prevent leakage\n",
        "    return df.with_columns(\n",
        "        (\n",
        "            (pl.col(target).shift(1).cum_sum().over(col).fill_null(0) + smoothing * global_mean) \n",
        "            / \n",
        "            (pl.col(target).shift(1).cum_count().over(col).fill_null(0) + smoothing)\n",
        "        ).alias(f\"{col}_enc\").cast(pl.Float32)\n",
        "    )\n",
        "\n",
        "print(\"Applying Smoothed Target Encoding (Iter E)...\")\n",
        "\n",
        "# Join into one for encoding, but keep it lean (ID columns only)\n",
        "cols_to_keep = [\"source_set\", \"feature_ch\", \"code\", \"sub_code\", \"ts_index\"]\n",
        "full_df = pl.concat([\n",
        "    train_df.select(cols_to_keep),\n",
        "    valid_df.select(cols_to_keep),\n",
        "    test_df.select(cols_to_keep)\n",
        "], how=\"diagonal\")\n",
        "\n",
        "for col in [\"code\", \"sub_code\"]:\n",
        "    full_df = create_smoothed_target_encoding_pl(full_df, col)\n",
        "    gc.collect()\n",
        "\n",
        "# Transfer encodings back using HSTACK (requires same order, which filter preserves)\n",
        "print(\"  Transferring encodings via hstack...\")\n",
        "train_enc = full_df.filter(pl.col(\"source_set\")==\"train\").select([\"code_enc\", \"sub_code_enc\"])\n",
        "train_df = train_df.hstack(train_enc)\n",
        "del train_enc\n",
        "\n",
        "valid_enc = full_df.filter(pl.col(\"source_set\")==\"valid\").select([\"code_enc\", \"sub_code_enc\"])\n",
        "valid_df = valid_df.hstack(valid_enc)\n",
        "del valid_enc\n",
        "\n",
        "test_enc = full_df.filter(pl.col(\"source_set\")==\"test\").select([\"code_enc\", \"sub_code_enc\"])\n",
        "test_df = test_df.hstack(test_enc)\n",
        "del test_enc\n",
        "\n",
        "del full_df\n",
        "gc.collect()\n",
        "\n",
        "features_e = features_d + [\"code_enc\", \"sub_code_enc\"]\n",
        "print(f\"âœ“ Target encoding complete. Total candidates: {len(features_e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-18-cb056bb2",
        "language": "markdown"
      },
      "source": [
        "# Step 8: Feature Selection (Iter F)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-19-b7815232",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Iteration F: Advanced Feature Selection (Model-Based)\n",
        "\n",
        "print(\"Applying Non-Linear Feature Selection (Iter F)...\")\n",
        "\n",
        "# 1. Create Interactions\n",
        "new_cols = []\n",
        "top_feats = [c for c in features_e if \"_lag_1\" in c or \"_roll_mean_7\" in c][:10]\n",
        "for feat in top_feats:\n",
        "    new_cols.append((pl.col(feat) * pl.col(\"horizon\")).alias(f\"{feat}_x_horizon\"))\n",
        "\n",
        "train_df = train_df.with_columns(new_cols)\n",
        "valid_df = valid_df.with_columns(new_cols)\n",
        "test_df = test_df.with_columns(new_cols)\n",
        "\n",
        "all_candidates_f = features_e + [f\"{feat}_x_horizon\" for feat in top_feats]\n",
        "all_candidates_f = [c for c in all_candidates_f if c in train_df.columns]\n",
        "\n",
        "# 2. Use LightGBM Importance for Selection\n",
        "X_sel_np = train_df.select(all_candidates_f).fill_null(0).to_numpy()\n",
        "y_sel_np = train_df[\"feature_ch\"].to_numpy()\n",
        "\n",
        "sel_model = lgb.LGBMRegressor(n_estimators=100, device=\"gpu\", random_state=42, verbose=-1)\n",
        "sel_model.fit(X_sel_np, y_sel_np)\n",
        "\n",
        "importance_df = pl.DataFrame({\n",
        "    \"feature\": all_candidates_f,\n",
        "    \"importance\": sel_model.feature_importances_\n",
        "}).sort(\"importance\", descending=True)\n",
        "\n",
        "selected_features_f = importance_df.head(150)[\"feature\"].to_list()\n",
        "print(f\"âœ“ Selected top {len(selected_features_f)} features\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-20-d5c54363",
        "language": "markdown"
      },
      "source": [
        "# Iteration G: Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-21-08c00070",
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Iteration G: The \"Silver Bullet\" Ensemble (Final Submission on test.parquet)\n",
        "\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "print(\"Training Integrated GPU Ensemble & Generating Final Submission...\")\n",
        "clear_gpu_memory()\n",
        "\n",
        "horizons = sorted(train_df[\"horizon\"].unique().to_list())\n",
        "preds_test = []\n",
        "preds_valid = []\n",
        "\n",
        "for h in horizons:\n",
        "    t_h = train_df.filter(pl.col(\"horizon\") == h)\n",
        "    v_h = valid_df.filter(pl.col(\"horizon\") == h)\n",
        "    te_h = test_df.filter(pl.col(\"horizon\") == h)\n",
        "    \n",
        "    if t_h.height == 0: continue\n",
        "    \n",
        "    print(f\"Processing Horizon {h}...\")\n",
        "    \n",
        "    X_train = t_h.select(selected_features_f).fill_null(0).to_numpy()\n",
        "    y_train = t_h[\"feature_ch\"].to_numpy()\n",
        "    w_train = t_h[\"feature_cg\"].fill_null(1.0).to_numpy()\n",
        "    \n",
        "    X_valid = v_h.select(selected_features_f).fill_null(0).to_numpy()\n",
        "    X_test = te_h.select(selected_features_f).fill_null(0).to_numpy()\n",
        "    \n",
        "    # Simple weighted blend\n",
        "    def train_and_predict(X_tr, y_tr, w_tr, X_val, X_te):\n",
        "        # LGBM\n",
        "        m1 = lgb.LGBMRegressor(n_estimators=500, device=\"gpu\", random_state=42, verbose=-1)\n",
        "        m1.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "        p1_v, p1_t = m1.predict(X_val), m1.predict(X_te)\n",
        "        \n",
        "        # XGB\n",
        "        m2 = xgb.XGBRegressor(n_estimators=500, tree_method=\"hist\", device=\"cuda\", random_state=42)\n",
        "        m2.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "        p2_v, p2_t = m2.predict(X_val), m2.predict(X_te)\n",
        "        \n",
        "        # CatBoost\n",
        "        m3 = CatBoostRegressor(n_estimators=500, task_type=\"GPU\", random_state=42, verbose=0)\n",
        "        m3.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "        p3_v, p3_t = m3.predict(X_val), m3.predict(X_te)\n",
        "        \n",
        "        clear_gpu_memory()\n",
        "        v_res = (0.4 * p1_v + 0.4 * p2_v + 0.2 * p3_v)\n",
        "        t_res = (0.4 * p1_t + 0.4 * p2_t + 0.2 * p3_t)\n",
        "        return v_res, t_res\n",
        "\n",
        "    p_val, p_test = train_and_predict(X_train, y_train, w_train, X_valid, X_test)\n",
        "    \n",
        "    preds_valid.append(v_h.select(\"id\").with_columns(pl.Series(\"prediction\", p_val)))\n",
        "    preds_test.append(te_h.select(\"id\").with_columns(pl.Series(\"prediction\", p_test)))\n",
        "\n",
        "# Save Submission\n",
        "if preds_test:\n",
        "    sub = pl.concat(preds_test)\n",
        "    sub.write_csv(\"submission_final_polars.csv\")\n",
        "    print(f\"âœ“ Saved submission with {sub.height} rows\")\n",
        "\n",
        "# Validation Score\n",
        "if preds_valid:\n",
        "    val_res = pl.concat(preds_valid)\n",
        "    val_merged = valid_df.join(val_res, on=\"id\")\n",
        "    y_true = cpu_to_gpu(val_merged[\"feature_ch\"].to_numpy())\n",
        "    y_pred = cpu_to_gpu(val_merged[\"prediction\"].to_numpy())\n",
        "    weights = cpu_to_gpu(val_merged[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "    score = weighted_rmse_score(y_true, y_pred, weights)\n",
        "    print(f\"ðŸš€ Final Validation Skill Score: {score:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
