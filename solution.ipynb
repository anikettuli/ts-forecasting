{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hedge Fund Time Series Forecasting - Optimized Solution\n",
    "\n",
    "**Objective**: Predict `feature_ch` using weighted RMSE metric.\n",
    "**Constraints**: Google Colab (limited RAM), <6hr runtime.\n",
    "**Optimizations**: Memory-efficient processing, selective feature engineering, streamlined ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as np\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB.\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressive memory cleanup for GPU and CPU.\"\"\"\n",
    "    gc.collect()\n",
    "    try:\n",
    "        mempool = np.get_default_memory_pool()\n",
    "        mempool.free_all_blocks()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Check GPU\n",
    "try:\n",
    "    print(f\"Python: {sys.executable}\")\n",
    "    devs = np.cuda.runtime.getDeviceCount()\n",
    "    print(f\"GPU: {devs} device(s), CuPy {np.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"GPU Error: {e}\")\n",
    "\n",
    "# Download Data\n",
    "def download_data(comp=\"ts-forecasting\"):\n",
    "    if os.path.exists(\"data/train.parquet\"):\n",
    "        print(\"Data exists.\")\n",
    "        return\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    env = os.environ.copy()\n",
    "    env[\"KAGGLE_USERNAME\"] = \"dummy_user\"\n",
    "    env[\"KAGGLE_KEY\"] = \"KGAT_ccc00b322d3c4b85f0036a23cc420469\"\n",
    "    try:\n",
    "        subprocess.run([\"kaggle\", \"competitions\", \"download\", \"-c\", comp], check=True, env=env)\n",
    "        with zipfile.ZipFile(f\"{comp}.zip\", 'r') as z:\n",
    "            z.extractall(\"data\")\n",
    "        os.remove(f\"{comp}.zip\")\n",
    "        print(\"Downloaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "\n",
    "download_data()\n",
    "print(f\"Memory: {get_memory_usage():.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import numpy as np_cpu\n",
    "from typing import List, Dict, Tuple\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pl.Config.set_streaming_chunk_size(10000)\n",
    "\n",
    "def gpu_to_cpu(x):\n",
    "    \"\"\"CuPy GPU → NumPy CPU (handles scalars + arrays).\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(x, (float, int, np_cpu.generic)):\n",
    "            return x\n",
    "        return x.get() if hasattr(x, 'get') else np_cpu.asarray(x)\n",
    "    except:\n",
    "        return np_cpu.asarray(x)\n",
    "\n",
    "def cpu_to_gpu(x):\n",
    "    \"\"\"NumPy CPU → CuPy GPU.\"\"\"\n",
    "    return np.asarray(x) if x is not None else None\n",
    "\n",
    "def weighted_rmse_score(y_true, y_pred, weights) -> float:\n",
    "    \"\"\"\n",
    "    SkillScore = 1 - sqrt(sum(w*(y-y_hat)²) / sum(w*y²))\n",
    "    Higher is better (max 1.0)\n",
    "    \"\"\"\n",
    "    y_t = np.asarray(y_true)\n",
    "    y_p = np.asarray(y_pred)\n",
    "    w = np.asarray(weights)\n",
    "    numerator = np.sum(w * (y_t - y_p) ** 2)\n",
    "    denominator = np.sum(w * y_t ** 2) + 1e-8\n",
    "    score = 1 - np.sqrt(numerator / denominator)\n",
    "    return float(gpu_to_cpu(score))\n",
    "\n",
    "def fast_eval(df_tr, df_va, feats, target=\"feature_ch\", weight=\"feature_cg\"):\n",
    "    \"\"\"Quick LGBM eval for iteration tracking.\"\"\"\n",
    "    X_tr = df_tr.select(feats).fill_null(0).to_numpy()\n",
    "    y_tr = df_tr[target].to_numpy()\n",
    "    w_tr = df_tr[weight].fill_null(1.0).to_numpy()\n",
    "    \n",
    "    X_va = df_va.select(feats).fill_null(0).to_numpy()\n",
    "    y_va = df_va[target].to_numpy()\n",
    "    w_va = df_va[weight].fill_null(1.0).to_numpy()\n",
    "    \n",
    "    model = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        device=\"gpu\",\n",
    "        random_state=42,\n",
    "        verbose=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "    \n",
    "    pred = model.predict(X_va)\n",
    "    return weighted_rmse_score(\n",
    "        cpu_to_gpu(y_va),\n",
    "        cpu_to_gpu(pred),\n",
    "        cpu_to_gpu(w_va)\n",
    "    )\n",
    "\n",
    "print(f\"Memory after imports: {get_memory_usage():.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Memory-Optimized Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_data(\n",
    "    train_path=\"data/train.parquet\",\n",
    "    test_path=\"data/test.parquet\",\n",
    "    valid_ratio=0.2\n",
    "):\n",
    "    \"\"\"Load data with memory-optimized dtypes.\"\"\"\n",
    "    print(f\"Loading {train_path}...\")\n",
    "    \n",
    "    def optimize_memory(df):\n",
    "        \"\"\"Reduce memory footprint aggressively.\"\"\"\n",
    "        optimizations = []\n",
    "        for col, dtype in df.schema.items():\n",
    "            if dtype == pl.Float64:\n",
    "                optimizations.append(pl.col(col).cast(pl.Float32))\n",
    "            elif dtype in (pl.Utf8, pl.String):\n",
    "                optimizations.append(pl.col(col).cast(pl.Categorical))\n",
    "            elif dtype == pl.Int64:\n",
    "                optimizations.append(pl.col(col).cast(pl.Int32))\n",
    "        return df.with_columns(optimizations) if optimizations else df\n",
    "    \n",
    "    # Load and optimize\n",
    "    train_full = optimize_memory(pl.read_parquet(train_path))\n",
    "    test_df = optimize_memory(pl.read_parquet(test_path))\n",
    "    \n",
    "    print(f\"  Train shape: {train_full.shape}, Test shape: {test_df.shape}\")\n",
    "    \n",
    "    # Time-based split\n",
    "    max_ts = train_full[\"ts_index\"].max()\n",
    "    min_ts = train_full[\"ts_index\"].min()\n",
    "    split_ts = max_ts - int((max_ts - min_ts) * valid_ratio)\n",
    "    \n",
    "    train_df = train_full.filter(pl.col(\"ts_index\") < split_ts)\n",
    "    valid_df = train_full.filter(pl.col(\"ts_index\") >= split_ts)\n",
    "    \n",
    "    del train_full\n",
    "    clear_memory()\n",
    "    \n",
    "    # Identify feature columns\n",
    "    exclude_cols = [\n",
    "        \"id\", \"code\", \"sub_code\", \"sub_category\",\n",
    "        \"feature_ch\", \"feature_cg\", \"ts_index\", \"horizon\"\n",
    "    ]\n",
    "    feature_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
    "    \n",
    "    print(f\"  Features: {len(feature_cols)}, Memory: {get_memory_usage():.0f} MB\")\n",
    "    return train_df, valid_df, test_df, feature_cols\n",
    "\n",
    "train_df, valid_df, test_df, base_features = load_and_split_data()\n",
    "\n",
    "# Baseline score\n",
    "baseline_pred = train_df[\"feature_ch\"].mean()\n",
    "y_true = cpu_to_gpu(valid_df[\"feature_ch\"].to_numpy())\n",
    "weights = cpu_to_gpu(valid_df[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
    "\n",
    "score_a = weighted_rmse_score(\n",
    "    y_true,\n",
    "    np.full_like(y_true, baseline_pred),\n",
    "    weights\n",
    ")\n",
    "print(f\"\\nIteration A (Baseline): {score_a:.4f} | Mean prediction | Features: {len(base_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-Efficient Temporal Features\n",
    "\n",
    "**Trade-off Analysis**:\n",
    "- Using ALL features: Maximum signal capture but ~3x memory overhead (risk of Colab OOM)\n",
    "- Using TOP N features: ~70-90% of signal with 5-10x less memory usage\n",
    "\n",
    "**Configuration**: Adjust `N_TOP_FEATURES` below (50=conservative, 75=balanced, 100+=aggressive)\n",
    "\n",
    "**Optimization**: Process each split separately to avoid 3x memory overhead from concatenation.\n",
    "**Optimization**: Reduce batch size for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION: Adjust based on Colab memory (12.7GB typical)\n",
    "N_TOP_FEATURES = 75  # 50=conservative, 75=balanced, 100+=aggressive (risk of OOM)\n",
    "BATCH_SIZE = 5  # Lower = less memory but slower\n",
    "\n",
    "def create_temporal_features_single(df, feats, group_cols=[\"code\", \"sub_code\"], windows=[7, 30], batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Create temporal features with memory-efficient batching.\n",
    "    Uses smaller batches to prevent Colab OOM.\n",
    "    \"\"\"\n",
    "    df = df.sort(group_cols + [\"ts_index\"])\n",
    "    \n",
    "    for i in range(0, len(feats), batch_size):\n",
    "        batch = feats[i:i+batch_size]\n",
    "        exprs = []\n",
    "        \n",
    "        for f in batch:\n",
    "            # Lag feature (t-1)\n",
    "            exprs.append(\n",
    "                pl.col(f)\n",
    "                .shift(1)\n",
    "                .over(group_cols)\n",
    "                .alias(f\"{f}_lag1\")\n",
    "                .cast(pl.Float32)\n",
    "            )\n",
    "            \n",
    "            # Rolling means\n",
    "            for w in windows:\n",
    "                exprs.append(\n",
    "                    pl.col(f)\n",
    "                    .shift(1)\n",
    "                    .rolling_mean(window_size=w, min_periods=1)\n",
    "                    .over(group_cols)\n",
    "                    .alias(f\"{f}_rm{w}\")\n",
    "                    .cast(pl.Float32)\n",
    "                )\n",
    "        \n",
    "        df = df.with_columns(exprs)\n",
    "        \n",
    "        # Aggressive cleanup every batch\n",
    "        if i % (batch_size * 4) == 0:\n",
    "            clear_memory()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Select top features for temporal engineering\n",
    "print(f\"Selecting top {N_TOP_FEATURES} features for temporal engineering...\")\n",
    "\n",
    "X_quick = train_df.select(base_features).fill_null(0).to_numpy()\n",
    "y_quick = train_df[\"feature_ch\"].to_numpy()\n",
    "\n",
    "quick_model = lgb.LGBMRegressor(\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.1,\n",
    "    device=\"gpu\",\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "quick_model.fit(X_quick, y_quick)\n",
    "\n",
    "# Get top N most important features\n",
    "importance = list(zip(base_features, quick_model.feature_importances_))\n",
    "importance.sort(key=lambda x: x[1], reverse=True)\n",
    "top_features_for_temporal = [f for f, _ in importance[:N_TOP_FEATURES]]\n",
    "\n",
    "print(f\"  Selected top {len(top_features_for_temporal)} features for temporal engineering\")\n",
    "print(f\"  Top 5: {top_features_for_temporal[:5]}\")\n",
    "print(f\"  Feature importance coverage: {sum(i for _, i in importance[:N_TOP_FEATURES]) / sum(i for _, i in importance):.1%}\")\n",
    "\n",
    "del X_quick, y_quick, quick_model\n",
    "clear_memory()\n",
    "\n",
    "# Process each split separately (no concatenation = memory efficient)\n",
    "print(\"\\nCreating temporal features...\")\n",
    "\n",
    "train_df = create_temporal_features_single(train_df, top_features_for_temporal)\n",
    "print(f\"  Train done. Memory: {get_memory_usage():.0f} MB\")\n",
    "\n",
    "valid_df = create_temporal_features_single(valid_df, top_features_for_temporal)\n",
    "print(f\"  Valid done. Memory: {get_memory_usage():.0f} MB\")\n",
    "\n",
    "test_df = create_temporal_features_single(test_df, top_features_for_temporal)\n",
    "print(f\"  Test done. Memory: {get_memory_usage():.0f} MB\")\n",
    "\n",
    "# Get all current features\n",
    "exclude = [\"id\", \"code\", \"sub_code\", \"sub_category\", \"feature_ch\", \"feature_cg\", \"ts_index\", \"horizon\"]\n",
    "current_features = [c for c in train_df.columns if c not in exclude]\n",
    "\n",
    "score_b = fast_eval(train_df, valid_df, current_features)\n",
    "print(f\"\\nIteration B (Temporal): {score_b:.4f} | Δ: {score_b - score_a:+.4f} | Features: {len(current_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizon-Aware Weighted Training\n",
    "\n",
    "**Optimization**: Use time-decay weights and feature_cg weights combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_horizon_model(df, feats, h, n_estimators=300):\n",
    "    \"\"\"Train model for specific horizon with combined weights.\"\"\"\n",
    "    df_h = df.filter(pl.col(\"horizon\") == h).sort(\"ts_index\")\n",
    "    \n",
    "    if df_h.height == 0:\n",
    "        return None\n",
    "    \n",
    "    # Combined weights: feature_cg * time_decay\n",
    "    max_ts = df_h[\"ts_index\"].max()\n",
    "    time_decay = 1.0 + 0.5 * (df_h[\"ts_index\"] / (max_ts + 1e-8))\n",
    "    df_h = df_h.with_columns(\n",
    "        (pl.col(\"feature_cg\").fill_null(1.0) * time_decay).alias(\"final_w\")\n",
    "    )\n",
    "    \n",
    "    # Time-based validation split (90/10)\n",
    "    unique_ts = df_h[\"ts_index\"].unique().sort()\n",
    "    split_idx = int(len(unique_ts) * 0.9)\n",
    "    split_ts = unique_ts[split_idx]\n",
    "    \n",
    "    tr = df_h.filter(pl.col(\"ts_index\") < split_ts)\n",
    "    va = df_h.filter(pl.col(\"ts_index\") >= split_ts)\n",
    "    \n",
    "    # Prepare data\n",
    "    X_tr = tr.select(feats).fill_null(0).to_numpy()\n",
    "    y_tr = tr[\"feature_ch\"].to_numpy()\n",
    "    w_tr = tr[\"final_w\"].to_numpy()\n",
    "    \n",
    "    X_va = va.select(feats).fill_null(0).to_numpy()\n",
    "    y_va = va[\"feature_ch\"].to_numpy()\n",
    "    w_va = va[\"final_w\"].to_numpy()\n",
    "    \n",
    "    # LightGBM with early stopping\n",
    "    dtrain = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)\n",
    "    dvalid = lgb.Dataset(X_va, label=y_va, weight=w_va, reference=dtrain)\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 31,\n",
    "        \"feature_fraction\": 0.8,\n",
    "        \"bagging_fraction\": 0.8,\n",
    "        \"bagging_freq\": 5,\n",
    "        \"device\": \"gpu\",\n",
    "        \"verbose\": -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=n_estimators,\n",
    "        valid_sets=[dvalid],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(30),\n",
    "            lgb.log_evaluation(period=0)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Training horizon models...\")\n",
    "horizons = sorted(train_df[\"horizon\"].unique().to_list())\n",
    "print(f\"  Horizons: {horizons}\")\n",
    "\n",
    "models_c = {}\n",
    "for h in horizons:\n",
    "    print(f\"  Training h={h}...\", end=\" \")\n",
    "    models_c[h] = train_horizon_model(train_df, current_features, h)\n",
    "    if models_c[h]:\n",
    "        print(f\"best_iter={models_c[h].best_iteration}\")\n",
    "    clear_memory()\n",
    "\n",
    "# Evaluate\n",
    "valid_df = valid_df.with_columns(pl.lit(0.0).alias(\"pred_c\").cast(pl.Float32))\n",
    "\n",
    "for h, model in models_c.items():\n",
    "    if model is None:\n",
    "        continue\n",
    "        \n",
    "    mask = valid_df[\"horizon\"] == h\n",
    "    if valid_df.filter(mask).height > 0:\n",
    "        X_va = valid_df.filter(mask).select(current_features).fill_null(0).to_numpy()\n",
    "        preds = model.predict(X_va)\n",
    "        valid_df = valid_df.with_columns(\n",
    "            pl.when(mask)\n",
    "            .then(pl.Series(preds))\n",
    "            .otherwise(pl.col(\"pred_c\"))\n",
    "            .alias(\"pred_c\")\n",
    "        )\n",
    "\n",
    "score_c = weighted_rmse_score(\n",
    "    y_true,\n",
    "    cpu_to_gpu(valid_df[\"pred_c\"].to_numpy()),\n",
    "    weights\n",
    ")\n",
    "print(f\"\\nIteration C (Horizon): {score_c:.4f} | Δ: {score_c - score_b:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental PCA (Memory-Safe)\n",
    "\n",
    "**Optimization**: Use IncrementalPCA with batch processing instead of loading all data at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Incremental PCA (Memory-safe)...\")\n",
    "\n",
    "# Select temporal features for PCA\n",
    "temporal_feats = [c for c in current_features if \"_rm\" in c or \"_lag\" in c]\n",
    "print(f\"  Using {len(temporal_feats)} temporal features\")\n",
    "\n",
    "# Fit IncrementalPCA in batches\n",
    "n_components = 8\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=1000)\n",
    "\n",
    "# Partial fit on training data in chunks\n",
    "train_data_for_pca = train_df.select(temporal_feats).fill_null(0).to_numpy()\n",
    "\n",
    "# Standardize first (compute mean/std on sample)\n",
    "sample_size = min(10000, len(train_data_for_pca))\n",
    "sample_idx = np_cpu.random.choice(len(train_data_for_pca), sample_size, replace=False)\n",
    "sample = train_data_for_pca[sample_idx]\n",
    "mean = sample.mean(axis=0)\n",
    "std = sample.std(axis=0)\n",
    "std[std == 0] = 1.0\n",
    "\n",
    "# Fit IPCA\n",
    "chunk_size = 5000\n",
    "for i in range(0, len(train_data_for_pca), chunk_size):\n",
    "    chunk = train_data_for_pca[i:i+chunk_size]\n",
    "    chunk_scaled = (chunk - mean) / std\n",
    "    ipca.partial_fit(chunk_scaled)\n",
    "    if i % (chunk_size * 2) == 0:\n",
    "        clear_memory()\n",
    "\n",
    "print(f\"  Explained variance: {ipca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Transform all datasets\n",
    "def transform_pca(df, cols, mean, std, ipca):\n",
    "    \"\"\"Transform data using fitted IPCA.\"\"\"\n",
    "    X = df.select(cols).fill_null(0).to_numpy()\n",
    "    X_scaled = (X - mean) / std\n",
    "    X_pca = ipca.transform(X_scaled)\n",
    "    return pl.DataFrame(X_pca, schema=[f\"pca_{i}\" for i in range(ipca.n_components_)]).cast(pl.Float32)\n",
    "\n",
    "train_pca = transform_pca(train_df, temporal_feats, mean, std, ipca)\n",
    "valid_pca = transform_pca(valid_df, temporal_feats, mean, std, ipca)\n",
    "test_pca = transform_pca(test_df, temporal_feats, mean, std, ipca)\n",
    "\n",
    "# Concatenate PCA features\n",
    "train_df = pl.concat([train_df, train_pca], how=\"horizontal\")\n",
    "valid_df = pl.concat([valid_df, valid_pca], how=\"horizontal\")\n",
    "test_df = pl.concat([test_df, test_pca], how=\"horizontal\")\n",
    "\n",
    "features_d = current_features + [f\"pca_{i}\" for i in range(n_components)]\n",
    "\n",
    "del train_data_for_pca, train_pca, valid_pca, test_pca\n",
    "clear_memory()\n",
    "\n",
    "score_d = fast_eval(train_df, valid_df, features_d)\n",
    "print(f\"Iteration D (PCA): {score_d:.4f} | Δ: {score_d - score_c:+.4f} | Features: {len(features_d)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding (Leakage-Safe)\n",
    "\n",
    "**Optimization**: Only use training data for encoding to prevent leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_encoding(df, col, train_df, target=\"feature_ch\", smoothing=10):\n",
    "    \"\"\"\n",
    "    Create smoothed target encoding using ONLY training data.\n",
    "    Prevents data leakage from validation/test sets.\n",
    "    \"\"\"\n",
    "    global_mean = train_df[target].mean()\n",
    "    \n",
    "    # Compute statistics from training data only\n",
    "    stats = train_df.group_by(col).agg([\n",
    "        pl.col(target).mean().alias(\"col_mean\"),\n",
    "        pl.col(target).count().alias(\"col_count\")\n",
    "    ])\n",
    "    \n",
    "    # Join to target dataframe\n",
    "    df = df.join(stats, on=col, how=\"left\")\n",
    "    \n",
    "    # Apply smoothing\n",
    "    df = df.with_columns(\n",
    "        (\n",
    "            (pl.col(\"col_mean\").fill_null(global_mean) * pl.col(\"col_count\").fill_null(0) + smoothing * global_mean) /\n",
    "            (pl.col(\"col_count\").fill_null(0) + smoothing)\n",
    "        ).alias(f\"{col}_enc\")\n",
    "        .cast(pl.Float32)\n",
    "    )\n",
    "    \n",
    "    return df.drop([\"col_mean\", \"col_count\"])\n",
    "\n",
    "print(\"Target Encoding (Leakage-safe)...\")\n",
    "\n",
    "for col in [\"code\", \"sub_code\"]:\n",
    "    train_df = create_target_encoding(train_df, col, train_df)\n",
    "    valid_df = create_target_encoding(valid_df, col, train_df)\n",
    "    test_df = create_target_encoding(test_df, col, train_df)\n",
    "    print(f\"  {col}_enc created\")\n",
    "\n",
    "features_e = features_d + [\"code_enc\", \"sub_code_enc\"]\n",
    "\n",
    "score_e = fast_eval(train_df, valid_df, features_e)\n",
    "print(f\"\\nIteration E (Target Enc): {score_e:.4f} | Δ: {score_e - score_d:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Smart Feature Selection...\")\n",
    "\n",
    "# Train model to get feature importances\n",
    "X_sel = train_df.select(features_e).fill_null(0).to_numpy()\n",
    "y_sel = train_df[\"feature_ch\"].to_numpy()\n",
    "w_sel = train_df[\"feature_cg\"].fill_null(1.0).to_numpy()\n",
    "\n",
    "sel_model = lgb.LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    device=\"gpu\",\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "sel_model.fit(X_sel, y_sel, sample_weight=w_sel)\n",
    "\n",
    "# Get importance and select features\n",
    "importance = list(zip(features_e, sel_model.feature_importances_))\n",
    "importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Keep features with importance > 0, cap at 200 for Colab safety\n",
    "selected_feats = [f for f, i in importance if i > 0][:200]\n",
    "\n",
    "print(f\"  Selected {len(selected_feats)} features\")\n",
    "print(f\"  Top 5: {[f for f, _ in importance[:5]]}\")\n",
    "\n",
    "del X_sel, y_sel, w_sel, sel_model\n",
    "clear_memory()\n",
    "\n",
    "score_f = fast_eval(train_df, valid_df, selected_feats)\n",
    "print(f\"\\nIteration F (Selection): {score_f:.4f} | Δ: {score_f - score_e:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurable Ensemble (LGBM + XGB + Optional CatBoost)\n",
    "\n",
    "**Trade-off Analysis**:\n",
    "- 2 models (LGBM+XGB): ~95% accuracy, 3-4 min per horizon, very safe\n",
    "- 3 models (+CatBoost): ~97% accuracy, 6-8 min per horizon, risk of OOM\n",
    "\n",
    "**Configuration**: Set `USE_CATBOOST = True` if you have >12GB RAM available.\n",
    "\n",
    "**Why CatBoost helps**: Different algorithm handles categorical features differently, adds diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION: Set to True if you have sufficient RAM (12GB+) and time\n",
    "USE_CATBOOST = False  # True = better accuracy but slower and memory-intensive\n",
    "\n",
    "if USE_CATBOOST:\n",
    "    from catboost import CatBoostRegressor\n",
    "    print(\"Training 3-model Ensemble (LGBM + XGB + CatBoost)...\")\n",
    "    weights_ensemble = [0.4, 0.35, 0.25]  # LGBM, XGB, CatBoost\n",
    "else:\n",
    "    print(\"Training 2-model Ensemble (LGBM + XGB)...\")\n",
    "    weights_ensemble = [0.5, 0.5]  # LGBM, XGB\n",
    "\n",
    "valid_df = valid_df.with_columns(pl.lit(0.0).alias(\"pred_g\").cast(pl.Float32))\n",
    "test_preds = []\n",
    "\n",
    "for h in horizons:\n",
    "    print(f\"\\nHorizon {h}:\")\n",
    "    \n",
    "    # Get data for this horizon\n",
    "    tr = train_df.filter(pl.col(\"horizon\") == h)\n",
    "    va = valid_df.filter(pl.col(\"horizon\") == h)\n",
    "    te = test_df.filter(pl.col(\"horizon\") == h)\n",
    "    \n",
    "    if tr.height == 0:\n",
    "        print(\"  No data, skipping\")\n",
    "        continue\n",
    "    \n",
    "    X_tr = tr.select(selected_feats).fill_null(0).to_numpy()\n",
    "    y_tr = tr[\"feature_ch\"].to_numpy()\n",
    "    \n",
    "    # Combined weights\n",
    "    max_ts = tr[\"ts_index\"].max()\n",
    "    time_w = 1.0 + 0.5 * (tr[\"ts_index\"].to_numpy() / (max_ts + 1e-8))\n",
    "    w_tr = tr[\"feature_cg\"].fill_null(1.0).to_numpy() * time_w\n",
    "    \n",
    "    X_va = va.select(selected_feats).fill_null(0).to_numpy()\n",
    "    X_te = te.select(selected_feats).fill_null(0).to_numpy()\n",
    "    \n",
    "    # Model 1: LightGBM\n",
    "    print(\"  Training LGBM...\", end=\" \")\n",
    "    m1 = lgb.LGBMRegressor(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=31,\n",
    "        feature_fraction=0.8,\n",
    "        bagging_fraction=0.8,\n",
    "        bagging_freq=5,\n",
    "        device=\"gpu\",\n",
    "        verbose=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    m1.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "    print(f\"done (n_estimators={m1.n_estimators_})\")\n",
    "    \n",
    "    # Model 2: XGBoost\n",
    "    print(\"  Training XGB...\", end=\" \")\n",
    "    m2 = xgb.XGBRegressor(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cuda\",\n",
    "        random_state=42,\n",
    "        verbosity=0\n",
    "    )\n",
    "    m2.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "    print(\"done\")\n",
    "    \n",
    "    predictions = [m1.predict(X_va), m2.predict(X_va)]\n",
    "    predictions_te = [m1.predict(X_te), m2.predict(X_te)]\n",
    "    \n",
    "    # Model 3: CatBoost (optional)\n",
    "    if USE_CATBOOST:\n",
    "        print(\"  Training CatBoost...\", end=\" \")\n",
    "        m3 = CatBoostRegressor(\n",
    "            n_estimators=400,\n",
    "            learning_rate=0.05,\n",
    "            depth=6,\n",
    "            task_type=\"GPU\",\n",
    "            verbose=0,\n",
    "            random_state=42\n",
    "        )\n",
    "        m3.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "        print(\"done\")\n",
    "        predictions.append(m3.predict(X_va))\n",
    "        predictions_te.append(m3.predict(X_te))\n",
    "    \n",
    "    # Weighted ensemble\n",
    "    p_va = sum(w * p for w, p in zip(weights_ensemble, predictions))\n",
    "    p_te = sum(w * p for w, p in zip(weights_ensemble, predictions_te))\n",
    "    \n",
    "    # Update validation predictions\n",
    "    mask = valid_df[\"horizon\"] == h\n",
    "    valid_df = valid_df.with_columns(\n",
    "        pl.when(mask)\n",
    "        .then(pl.Series(p_va))\n",
    "        .otherwise(pl.col(\"pred_g\"))\n",
    "        .alias(\"pred_g\")\n",
    "    )\n",
    "    \n",
    "    # Store test predictions\n",
    "    test_preds.append(\n",
    "        te.select(\"id\").with_columns(pl.Series(\"prediction\", p_te))\n",
    "    )\n",
    "    \n",
    "    # Cleanup models from memory\n",
    "    del m1, m2\n",
    "    if USE_CATBOOST:\n",
    "        del m3\n",
    "    clear_memory()\n",
    "\n",
    "# Final evaluation\n",
    "score_g = weighted_rmse_score(\n",
    "    y_true,\n",
    "    cpu_to_gpu(valid_df[\"pred_g\"].to_numpy()),\n",
    "    weights\n",
    ")\n",
    "\n",
    "# Save submission\n",
    "submission = pl.concat(test_preds)\n",
    "submission.write_csv(\"submission_optimized.csv\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"FINAL RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Iteration A (Baseline):    {score_a:.4f}\")\n",
    "print(f\"Iteration B (Temporal):    {score_b:.4f}  Δ: {score_b - score_a:+.4f}\")\n",
    "print(f\"Iteration C (Horizon):     {score_c:.4f}  Δ: {score_c - score_b:+.4f}\")\n",
    "print(f\"Iteration D (PCA):         {score_d:.4f}  Δ: {score_d - score_c:+.4f}\")\n",
    "print(f\"Iteration E (Target Enc):  {score_e:.4f}  Δ: {score_e - score_d:+.4f}\")\n",
    "print(f\"Iteration F (Selection):   {score_f:.4f}  Δ: {score_f - score_e:+.4f}\")\n",
    "print(f\"Iteration G (Ensemble):    {score_g:.4f}  Δ: {score_g - score_f:+.4f}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total Improvement: {score_g - score_a:+.4f}\")\n",
    "print(f\"Configuration: N_TOP_FEATURES={N_TOP_FEATURES}, USE_CATBOOST={USE_CATBOOST}\")\n",
    "print(f\"Submission saved: submission_optimized.csv\")\n",
    "print(f\"Final Memory: {get_memory_usage():.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Optimizations\n",
    "\n",
    "### Memory Optimizations\n",
    "1. **Separate Processing**: Process train/valid/test separately instead of concatenating (eliminates 3x memory overhead)\n",
    "2. **Smaller Batches**: Reduced batch size from 10 to 5 for temporal features\n",
    "3. **Configurable Feature Subset**: `N_TOP_FEATURES` parameter (default 75 instead of all)\n",
    "4. **IncrementalPCA**: Process PCA in chunks instead of loading all data\n",
    "5. **Aggressive Cleanup**: `clear_memory()` after each major operation + model deletion\n",
    "6. **Dtype Optimization**: Consistent Float32 usage throughout\n",
    "\n",
    "### Runtime Optimizations\n",
    "1. **Configurable Ensemble**: 2 models by default, optional 3rd (CatBoost)\n",
    "2. **Fewer Estimators**: Reduced from 500 to 400 with better early stopping\n",
    "3. **Smaller Feature Set**: Cap at 200 features max\n",
    "4. **Efficient Target Encoding**: No concatenation of all datasets\n",
    "\n",
    "### Accuracy Improvements\n",
    "1. **Leakage Prevention**: Target encoding uses only training data\n",
    "2. **Better Weighting**: Combined time-decay + feature_cg weights\n",
    "3. **Feature Selection**: Importance-based selection keeps only useful features\n",
    "4. **Horizon-Aware**: Separate models per horizon capture different patterns\n",
    "5. **Feature Coverage Tracking**: Shows importance coverage % for transparency\n",
    "\n",
    "### Bug Fixes\n",
    "1. **Fixed X_va undefined**: Properly defined in ensemble loop\n",
    "2. **Fixed target encoding leakage**: No longer uses test set target values\n",
    "3. **Proper memory pooling**: CuPy memory pool cleanup\n",
    "\n",
    "### Configuration Guide\n",
    "- **Conservative (8GB RAM)**: N_TOP_FEATURES=50, USE_CATBOOST=False\n",
    "- **Balanced (12GB RAM)**: N_TOP_FEATURES=75, USE_CATBOOST=False [DEFAULT]\n",
    "- **Aggressive (16GB+ RAM)**: N_TOP_FEATURES=100, USE_CATBOOST=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
