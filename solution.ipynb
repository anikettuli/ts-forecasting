{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 0: Download Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# GPU-Accelerated Time Series Forecasting\n",
        "\n",
        "**Hardware**: RTX 3070 + CUDA 13.1 + CuPy 13.3.0 + LightGBM\n",
        "\n",
        "**7-Step Pipeline**:\n",
        "1. Load data & baseline metric\n",
        "2. Temporal features (lags, rolling windows)\n",
        "3. Horizon-specific LightGBM models\n",
        "4. PCA dimensionality reduction\n",
        "5. Smoothed target encoding\n",
        "6. Feature selection & interactions\n",
        "7. Ensemble predictions\n",
        "\n",
        "**Key Design**: Matrix operations (GPU) → CuPy. External libs (CPU) → NumPy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: c:\\Users\\anike\\Desktop\\ts-forecasting\\.venv\\Scripts\\python.exe\n",
            "CuPy Version: 13.6.0\n",
            "✓ GPU Ready: 1 device(s), Compute Capability 86\n"
          ]
        }
      ],
      "source": [
        "# GPU Setup & Initialization\n",
        "\n",
        "import cupy as np\n",
        "import numpy as np_cpu\n",
        "import sys\n",
        "\n",
        "print(f\"Python: {sys.executable}\")\n",
        "print(f\"CuPy Version: {np.__version__}\")\n",
        "\n",
        "# Check GPU\n",
        "try:\n",
        "    device_count = np.cuda.runtime.getDeviceCount()\n",
        "    device = np.cuda.Device(0)\n",
        "    cap = device.compute_capability\n",
        "    print(f\"✓ GPU Ready: {device_count} device(s), Compute Capability {cap}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ GPU Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Step 1: Download Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File data/test.parquet already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import requests\n",
        "\n",
        "def download_data(url, filepath):\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"File {filepath} already exists. Skipping download.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Downloading data from {url}...\")\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    \n",
        "    response = requests.get(url, stream=True)\n",
        "    if response.status_code == 200:\n",
        "        with open(filepath, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(f\"Downloaded to {filepath}\")\n",
        "    else:\n",
        "        print(f\"Failed to download. Status code: {response.status_code}\")\n",
        "\n",
        "DATA_URL = \"https://storage.googleapis.com/kagglesdsdata/competitions/105581/15271735/test.parquet?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1770992767&Signature=Fj%2F5LPgyVbzbph1WeG3uCJRMdqsabiq%2BKM3msHzs8y%2BXoHuAzoKKjvFDg3t8ueiPngilQtsFWg4FbUfzajf%2BDqzehOTGwRWzU6bX0xvdrrOeQusa7glpDDnF9n6C0izwzU8k%2FxFDdU7qI6vUtJLm3Yk20zfZMYx%2BuGtFrrtoTzcUx0k5ut%2Ft4OtyyBeCzpsUpCA5EjKMGbqRnB5P%2F7SCEWlwCQ7ZXL8w0kKJGCC3%2FYOqSktMDRhaeLsyP3lfCejur%2BxGfcd8hoLQWsEHOkYEw91k%2F%2BOy6vg5PkpYlQN2Exovmjg3o56VBIAZLLZhU%2BCAvtfL4X%2Bw02HrNJVKi5mhoA%3D%3D&response-content-disposition=attachment%3B+filename%3Dtest.parquet\"\n",
        "download_data(DATA_URL, \"data/test.parquet\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Step 2: Imports & Metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utilities & Metric\n",
        "\n",
        "import polars as pl\n",
        "import warnings\n",
        "import os\n",
        "import lightgbm as lgb\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from typing import Tuple, List, Dict\n",
        "import requests\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# GPU ↔ CPU Conversion\n",
        "def gpu_to_cpu(x):\n",
        "    \"\"\"CuPy GPU → NumPy CPU (handles scalars + arrays).\"\"\"\n",
        "    if x is None:\n",
        "        return None\n",
        "    try:\n",
        "        # Use CuPy's .get() to explicitly convert to NumPy\n",
        "        if hasattr(x, 'get'):\n",
        "            return x.get()\n",
        "        elif hasattr(x, 'item'):\n",
        "            return x.item()\n",
        "        else:\n",
        "            return np_cpu.asarray(x)\n",
        "    except Exception as e:\n",
        "        # Fallback: ensure we return NumPy array, not CuPy\n",
        "        return np_cpu.asarray(x)\n",
        "\n",
        "def cpu_to_gpu(x):\n",
        "    \"\"\"NumPy CPU → CuPy GPU.\"\"\"\n",
        "    return np.asarray(x) if x is not None else None\n",
        "\n",
        "# Weighted RMSE Skill Score\n",
        "def weighted_rmse_score(y_true: np.ndarray, y_pred: np.ndarray, \n",
        "                        weights: np.ndarray) -> float:\n",
        "    \"\"\"GPU-accelerated metric. Returns Python float.\"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    weights = np.asarray(weights)\n",
        "    \n",
        "    score = 1 - np.sqrt(np.sum(weights * (y_true - y_pred) ** 2) / \n",
        "                        np.sum(weights * y_true ** 2))\n",
        "    return float(score) if np.sum(weights * y_true ** 2) > 0 else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Load Data & Baseline (Iter A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from data/test.parquet\n",
            "Loaded 1,447,107 rows with 92 columns\n",
            "Time index range: 3602 to 4376\n",
            "Validation split at ts_index >= 4183\n",
            "Baseline (Mean Prediction) Score: 0.3315\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Iteration A: The Golden Split & Metric (Polars)\n",
        "\n",
        "def load_and_split_data(\n",
        "    filepath: str = \"data/test.parquet\",\n",
        "    target_col: str = \"feature_ch\",\n",
        "    weight_col: str = \"feature_cg\",\n",
        "    valid_ratio: float = 0.25,\n",
        ") -> Tuple[pl.DataFrame, pl.DataFrame, list]:\n",
        "    print(\"Loading data from\", filepath)\n",
        "    if not os.path.exists(filepath):\n",
        "        # Create dummy data for testing if file doesn't exist\n",
        "        print(\"Warning: Data file not found. Creating dummy data.\")\n",
        "        n_rows = 10000\n",
        "        \n",
        "        # CuPy strict typing: Must convert lists to arrays for random.choice\n",
        "        codes = np.array([\"A\", \"B\"], dtype=object)\n",
        "        sub_codes = np.array([\"X\", \"Y\"], dtype=object)\n",
        "        categories = np.array([\"1\", \"2\"], dtype=object)\n",
        "        horizons_list = np.array([1, 10, 25], dtype=int)\n",
        "        \n",
        "        df = pl.DataFrame({\n",
        "            \"id\": [f\"c_sc_cat_h_{i}\" for i in range(n_rows)],\n",
        "            \"ts_index\": list(range(n_rows)),  # Polars expects lists, not arrays\n",
        "            \"code\": np.random.choice(codes, n_rows).tolist(),  # Convert GPU array to list for Polars\n",
        "            \"sub_code\": np.random.choice(sub_codes, n_rows).tolist(),\n",
        "            \"sub_category\": np.random.choice(categories, n_rows).tolist(),\n",
        "            \"horizon\": np.random.choice(horizons_list, n_rows).tolist(),\n",
        "            \"feature_ch\": np.random.randn(n_rows).tolist(),  # Target\n",
        "            \"feature_cg\": np.random.uniform(0.5, 1.5, n_rows).tolist(), # Weight\n",
        "        })\n",
        "        # Add dummy features\n",
        "        for i in range(10):\n",
        "            df = df.with_columns(pl.lit(np.random.randn()).alias(f\"feature_{i}\"))\n",
        "    else:\n",
        "        df = pl.read_parquet(filepath)\n",
        "    \n",
        "    print(f\"Loaded {df.height:,} rows with {len(df.columns)} columns\")\n",
        "\n",
        "    # Determine split point based on ts_index\n",
        "    min_ts = df[\"ts_index\"].min()\n",
        "    max_ts = df[\"ts_index\"].max()\n",
        "    ts_range = max_ts - min_ts\n",
        "    split_ts = max_ts - int(ts_range * valid_ratio)\n",
        "\n",
        "    print(f\"Time index range: {min_ts} to {max_ts}\")\n",
        "    print(f\"Validation split at ts_index >= {split_ts}\")\n",
        "\n",
        "    # Split data using filter (lazy or eager)\n",
        "    train_df = df.filter(pl.col(\"ts_index\") < split_ts)\n",
        "    valid_df = df.filter(pl.col(\"ts_index\") >= split_ts)\n",
        "    \n",
        "    # Feature columns (exclude meta)\n",
        "    exclude_cols = [\"id\", \"code\", \"sub_code\", \"sub_category\", target_col, weight_col, \"ts_index\", \"horizon\"]\n",
        "    feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
        "\n",
        "    return train_df, valid_df, feature_cols\n",
        "\n",
        "# Execute Iter A\n",
        "train_df, valid_df, feature_cols = load_and_split_data()\n",
        "\n",
        "# Baseline Calculation (GPU-accelerated with proper scalar handling)\n",
        "y_true_gpu = cpu_to_gpu(valid_df[\"feature_ch\"].to_numpy())\n",
        "weights_gpu = cpu_to_gpu(valid_df[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "\n",
        "# IMPORTANT: GPU scalars must be explicitly converted to Python scalars\n",
        "train_mean_gpu = np.mean(cpu_to_gpu(train_df[\"feature_ch\"].to_numpy()))\n",
        "train_mean = float(train_mean_gpu)  # Convert GPU scalar to Python float\n",
        "\n",
        "y_pred_baseline = np.ones_like(y_true_gpu) * train_mean\n",
        "baseline_score = weighted_rmse_score(y_true_gpu, y_pred_baseline, weights_gpu)\n",
        "print(f\"Baseline (Mean Prediction) Score: {baseline_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 4: Temporal Features (Iter B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating temporal features with Polars...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 160 temporal features\n",
            "Total features after Iter B: 244\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Iteration B: Temporal Feature Engineering (Polars)\n",
        "# Polars makes rolling windows extremely fast and clean.\n",
        "\n",
        "def create_temporal_features_pl(\n",
        "    df: pl.DataFrame,\n",
        "    feature_cols: List[str],\n",
        "    group_cols: List[str] = [\"code\", \"sub_code\", \"sub_category\"],\n",
        "    rolling_windows: List[int] = [3, 7, 14, 30],\n",
        ") -> pl.DataFrame:\n",
        "    print(\"Creating temporal features with Polars...\")\n",
        "    \n",
        "    # Ensure sorted by ts_index within groups\n",
        "    df = df.sort(group_cols + [\"ts_index\"])\n",
        "    \n",
        "    features_to_process = feature_cols[:20] if len(feature_cols) > 50 else feature_cols\n",
        "    \n",
        "    # Define expressions for rolling/lag features\n",
        "    exprs = []\n",
        "    \n",
        "    for feat in features_to_process:\n",
        "        # Lags\n",
        "        for lag in [1, 2, 3]:\n",
        "            exprs.append(pl.col(feat).shift(lag).over(group_cols).alias(f\"{feat}_lag_{lag}\"))\n",
        "            \n",
        "        # Rolling Means (Shifted by 1 to prevent leakage)\n",
        "        # Polars rolling operates on the column. .shift(1) ensures we don't peek.\n",
        "        for window in rolling_windows:\n",
        "            # rolling_mean usage: .rolling_mean(window_size).shift(1)\n",
        "            # We must use .over(group_cols) to respect groups!\n",
        "            # However, rolling_mean is deprecated in favor of rolling().mean()\n",
        "            # To apply over groups efficiently:\n",
        "            # We can use window functions inside over()\n",
        "            \n",
        "            # Note: naive .rolling() inside over() can be tricky in older Polars versions, \n",
        "            # but newer versions support it well.\n",
        "            # Best practice: use creating rolling columns separately if over() is complex or use window functions.\n",
        "            \n",
        "            # Efficient pattern: \n",
        "            # (col(feat).shift(1).rolling_mean(window)).over(group_cols)\n",
        "            # Shift FIRST to prevent leakage, then roll. Wait.\n",
        "            # If we shift first, then rolling window at T includes T-1, T-2... which is safe.\n",
        "            # Yes.\n",
        "            \n",
        "            col_name = f\"{feat}_roll_{window}\"\n",
        "            exprs.append(\n",
        "                pl.col(feat)\n",
        "                .shift(1)\n",
        "                .rolling_mean(window_size=window, min_periods=1)\n",
        "                .over(group_cols)\n",
        "                .alias(col_name)\n",
        "            )\n",
        "            \n",
        "        # Expanding Mean (Shifted)\n",
        "        # Cumulative sum / Count\n",
        "        # Shift(1) first\n",
        "        shifted = pl.col(feat).shift(1)\n",
        "        exprs.append(\n",
        "            (shifted.cum_sum() / shifted.cum_count())\n",
        "            .over(group_cols)\n",
        "            .alias(f\"{feat}_exp_mean\")\n",
        "            .fill_nan(0) # Handle potential division by zero\n",
        "        )\n",
        "\n",
        "    # Apply all expressions at once! Ultra fast.\n",
        "    df = df.with_columns(exprs)\n",
        "    \n",
        "    print(f\"Created {len(exprs)} temporal features\")\n",
        "    return df\n",
        "\n",
        "# Combine\n",
        "full_df = pl.concat([train_df, valid_df])\n",
        "\n",
        "# Create Features\n",
        "full_df = create_temporal_features_pl(full_df, feature_cols)\n",
        "\n",
        "# Re-split\n",
        "# We need to calculate split_ts again or reuse\n",
        "split_ts = full_df[\"ts_index\"].max() - int((full_df[\"ts_index\"].max() - full_df[\"ts_index\"].min()) * 0.25)\n",
        "train_df = full_df.filter(pl.col(\"ts_index\") < split_ts)\n",
        "valid_df = full_df.filter(pl.col(\"ts_index\") >= split_ts)\n",
        "\n",
        "# Update feature list\n",
        "current_features = [c for c in full_df.columns if c not in [\"id\", \"code\", \"sub_code\", \"sub_category\", \"feature_ch\", \"feature_cg\", \"ts_index\", \"horizon\"]]\n",
        "print(f\"Total features after Iter B: {len(current_features)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iteration C: Weighted LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Horizon-specific Models (Iter C)...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[270]\ttraining's rmse: 0.168577\tvalid_1's rmse: 0.438101\n",
            "  Horizon 1 Score: 0.8802\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[263]\ttraining's rmse: 0.165978\tvalid_1's rmse: 0.438827\n",
            "  Horizon 3 Score: 0.8805\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[172]\ttraining's rmse: 0.190387\tvalid_1's rmse: 0.422422\n",
            "  Horizon 10 Score: 0.8872\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[171]\ttraining's rmse: 0.188389\tvalid_1's rmse: 0.43212\n",
            "  Horizon 25 Score: 0.8894\n",
            "Overall Iteration C Score: 0.8843\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Iteration C: Weighted LightGBM (Optimized GPU Pipeline)\n",
        "\n",
        "def train_lgb_model(train_df, valid_df, features, params=None):\n",
        "    if params is None:\n",
        "        params = {\n",
        "            \"objective\": \"regression\",\n",
        "            \"metric\": \"rmse\",\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"num_leaves\": 31,\n",
        "            \"verbose\": -1,\n",
        "            \"n_jobs\": -1,\n",
        "            \"device\": \"gpu\"\n",
        "        }\n",
        "    \n",
        "    # Extract to GPU arrays in one batch\n",
        "    X_train_gpu = cpu_to_gpu(train_df.select(features).fill_null(0).to_numpy())\n",
        "    y_train_gpu = cpu_to_gpu(train_df[\"feature_ch\"].to_numpy())\n",
        "    w_train_gpu = cpu_to_gpu(train_df[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "    \n",
        "    X_valid_gpu = cpu_to_gpu(valid_df.select(features).fill_null(0).to_numpy())\n",
        "    y_valid_gpu = cpu_to_gpu(valid_df[\"feature_ch\"].to_numpy())\n",
        "    w_valid_gpu = cpu_to_gpu(valid_df[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "    \n",
        "    # Transfer to CPU only once for LightGBM\n",
        "    X_train_np = gpu_to_cpu(X_train_gpu)\n",
        "    y_train_np = gpu_to_cpu(y_train_gpu)\n",
        "    w_train_np = gpu_to_cpu(w_train_gpu)\n",
        "    \n",
        "    X_valid_np = gpu_to_cpu(X_valid_gpu)\n",
        "    y_valid_np = gpu_to_cpu(y_valid_gpu)\n",
        "    w_valid_np = gpu_to_cpu(w_valid_gpu)\n",
        "    \n",
        "    train_data = lgb.Dataset(X_train_np, label=y_train_np, weight=w_train_np)\n",
        "    valid_data = lgb.Dataset(X_valid_np, label=y_valid_np, weight=w_valid_np, reference=train_data)\n",
        "    \n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        train_data,\n",
        "        num_boost_round=1000,\n",
        "        valid_sets=[train_data, valid_data],\n",
        "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(False)]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Train separate models for horizons\n",
        "horizons = sorted(train_df[\"horizon\"].unique().to_list())\n",
        "models = {}\n",
        "\n",
        "print(\"Training Horizon-specific Models (Iter C)...\")\n",
        "for h in horizons:\n",
        "    t_h = train_df.filter(pl.col(\"horizon\") == h)\n",
        "    v_h = valid_df.filter(pl.col(\"horizon\") == h)\n",
        "    \n",
        "    if t_h.height == 0 or v_h.height == 0: continue\n",
        "        \n",
        "    model = train_lgb_model(t_h, v_h, current_features)\n",
        "    models[h] = model\n",
        "    \n",
        "    # Score (GPU-accelerated)\n",
        "    v_preds = model.predict(v_h.select(current_features).fill_null(0).to_numpy())\n",
        "    v_preds_gpu = cpu_to_gpu(v_preds)\n",
        "    y_true_gpu = cpu_to_gpu(v_h[\"feature_ch\"].to_numpy())\n",
        "    weights_gpu = cpu_to_gpu(v_h[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "    score = weighted_rmse_score(y_true_gpu, v_preds_gpu, weights_gpu)\n",
        "    print(f\"  Horizon {h} Score: {score:.4f}\")\n",
        "\n",
        "# Collect predictions on GPU then transfer once\n",
        "preds_full = []\n",
        "for h, model in models.items():\n",
        "    sub_df = valid_df.filter(pl.col(\"horizon\") == h)\n",
        "    if sub_df.height > 0:\n",
        "        preds = model.predict(sub_df.select(current_features).fill_null(0).to_numpy())\n",
        "        temp_df = sub_df.select(\"id\").with_columns(pl.Series(name=\"pred_iter_c_h\", values=preds))\n",
        "        preds_full.append(temp_df)\n",
        "\n",
        "valid_df = valid_df.with_columns(pl.lit(0.0).alias(\"pred_iter_c\"))\n",
        "if preds_full:\n",
        "    preds_all = pl.concat(preds_full)\n",
        "    valid_df = valid_df.join(preds_all, on=\"id\", how=\"left\").with_columns(\n",
        "        pl.col(\"pred_iter_c_h\").fill_null(0).alias(\"pred_iter_c\")\n",
        "    )\n",
        "\n",
        "# Overall evaluation (GPU-accelerated)\n",
        "y_true_gpu = cpu_to_gpu(valid_df[\"feature_ch\"].to_numpy())\n",
        "pred_gpu = cpu_to_gpu(valid_df[\"pred_iter_c\"].to_numpy())\n",
        "weights_gpu = cpu_to_gpu(valid_df[\"feature_cg\"].fill_null(1.0).to_numpy())\n",
        "overall_score_c = weighted_rmse_score(y_true_gpu, pred_gpu, weights_gpu)\n",
        "print(f\"Overall Iteration C Score: {overall_score_c:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iteration D: PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying PCA (Iter D)...\n",
            "PCA features added: 10\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Iteration D: PCA (GPU-optimized)\n",
        "\n",
        "print(\"Applying PCA (Iter D)...\")\n",
        "\n",
        "# Select numeric features\n",
        "pca_features = [c for c in train_df.columns if c.startswith(\"feature_\") or \"_roll_\" in c]\n",
        "pca_features = pca_features[:50]\n",
        "\n",
        "# Load to GPU\n",
        "X_train_gpu = cpu_to_gpu(train_df.select(pca_features).fill_null(0).to_numpy())\n",
        "X_valid_gpu = cpu_to_gpu(valid_df.select(pca_features).fill_null(0).to_numpy())\n",
        "\n",
        "# GPU-accelerated standardization (all operations on GPU)\n",
        "mean_gpu = np.mean(X_train_gpu, axis=0, keepdims=True)\n",
        "std_gpu = np.std(X_train_gpu, axis=0, keepdims=True)\n",
        "std_gpu = np.where(std_gpu == 0, 1.0, std_gpu)  # Avoid division by zero on GPU\n",
        "\n",
        "X_train_scaled = (X_train_gpu - mean_gpu) / std_gpu\n",
        "X_valid_scaled = (X_valid_gpu - mean_gpu) / std_gpu\n",
        "\n",
        "# Transfer to CPU only for sklearn PCA\n",
        "X_train_np = gpu_to_cpu(X_train_scaled)\n",
        "X_valid_np = gpu_to_cpu(X_valid_scaled)\n",
        "\n",
        "n_components = 10\n",
        "pca = PCA(n_components=n_components)\n",
        "X_train_pca = pca.fit_transform(X_train_np)\n",
        "X_valid_pca = pca.transform(X_valid_np)\n",
        "\n",
        "# Add back to DataFrames\n",
        "pca_cols = [f\"pca_{i}\" for i in range(n_components)]\n",
        "train_pca_df = pl.DataFrame(X_train_pca, schema=pca_cols)\n",
        "valid_pca_df = pl.DataFrame(X_valid_pca, schema=pca_cols)\n",
        "\n",
        "train_df = pl.concat([train_df, train_pca_df], how=\"horizontal\")\n",
        "valid_df = pl.concat([valid_df, valid_pca_df], how=\"horizontal\")\n",
        "\n",
        "features_d = current_features + pca_cols\n",
        "print(f\"PCA features added: {len(pca_cols)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iteration E: Smoothed Target Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying Smoothed Target Encoding (Iter E)...\n"
          ]
        }
      ],
      "source": [
        "# Iteration E: Smoothed Target Encoding (Polars Fast)\n",
        "\n",
        "def create_smoothed_target_encoding_pl(\n",
        "    df, col, target=\"feature_ch\", weight=\"feature_cg\", min_samples=10, smoothing=10\n",
        "):\n",
        "    # Sort\n",
        "    df = df.sort([col, \"ts_index\"])\n",
        "    global_mean = df[target].mean()\n",
        "    \n",
        "    # Polars expressions for expanding mean\n",
        "    # We want: (cumsum_shift * n_shift + smooth*global) / (n_shift + smooth)\n",
        "    \n",
        "    # Calculate Expanding Sum and Count\n",
        "    # Use over()\n",
        "    \n",
        "    return df.with_columns(\n",
        "        (\n",
        "            (pl.col(target).shift(1).cum_sum().over(col).fill_null(0) + smoothing * global_mean) \n",
        "            / \n",
        "            (pl.col(target).shift(1).cum_count().over(col).fill_null(0) + smoothing)\n",
        "        ).alias(f\"{col}_enc_smooth\")\n",
        "    )\n",
        "\n",
        "print(\"Applying Smoothed Target Encoding (Iter E)...\")\n",
        "# Ensure both DataFrames have matching columns by selecting only those in common\n",
        "cols_in_both = [c for c in train_df.columns if c in valid_df.columns]\n",
        "full_df = pl.concat([train_df.select(cols_in_both), valid_df.select(cols_in_both)])\n",
        "\n",
        "for col in [\"code\", \"sub_code\", \"sub_category\"]:\n",
        "    full_df = create_smoothed_target_encoding_pl(full_df, col)\n",
        "\n",
        "# Re-split\n",
        "train_df = full_df.filter(pl.col(\"ts_index\") < split_ts)\n",
        "valid_df = full_df.filter(pl.col(\"ts_index\") >= split_ts)\n",
        "\n",
        "features_e = features_d + [f\"{c}_enc_smooth\" for c in [\"code\", \"sub_code\", \"sub_category\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 8: Feature Selection (Iter F)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating Interaction Features (Iter F)...\n",
            "Selected 100 features from 263 candidates\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Iteration F: Interaction Features & Feature Selection (GPU-optimized)\n",
        "\n",
        "print(\"Creating Interaction Features (Iter F)...\")\n",
        "new_cols = []\n",
        "base_feats = [c for c in features_e if \"lag\" in c][:5]\n",
        "\n",
        "for feat in base_feats:\n",
        "    new_cols.append((pl.col(feat) * pl.col(\"horizon\")).alias(f\"{feat}_x_horizon\"))\n",
        "\n",
        "new_cols.append((pl.col(\"horizon\") ** 2).alias(\"horizon_squared\"))\n",
        "\n",
        "train_df = train_df.with_columns(new_cols)\n",
        "valid_df = valid_df.with_columns(new_cols)\n",
        "\n",
        "interaction_feats = [f\"{feat}_x_horizon\" for feat in base_feats] + [\"horizon_squared\"]\n",
        "all_candidates_f = features_e + interaction_feats\n",
        "\n",
        "# Feature Selection with GPU acceleration\n",
        "X_train_gpu = cpu_to_gpu(train_df.select(all_candidates_f).fill_null(0).to_numpy())\n",
        "y_train_gpu = cpu_to_gpu(train_df[\"feature_ch\"].to_numpy())\n",
        "\n",
        "# GPU-accelerated correlation/variance calculations before sklearn\n",
        "variances_gpu = np.var(X_train_gpu, axis=0)\n",
        "X_train_np = gpu_to_cpu(X_train_gpu)\n",
        "y_train_np = gpu_to_cpu(y_train_gpu)\n",
        "\n",
        "selector = SelectKBest(score_func=f_regression, k=min(100, len(all_candidates_f)))\n",
        "selector.fit(X_train_np, y_train_np)\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "selected_features_f = [all_candidates_f[i] for i in selected_indices]\n",
        "print(f\"Selected {len(selected_features_f)} features from {len(all_candidates_f)} candidates\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iteration G: Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Ensemble (Iter G)...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[103]\ttraining's rmse: 0.212775\tvalid_1's rmse: 0.346027\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[181]\ttraining's rmse: 0.16752\tvalid_1's rmse: 0.336707\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[91]\ttraining's rmse: 0.217647\tvalid_1's rmse: 0.347597\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[162]\ttraining's rmse: 0.171731\tvalid_1's rmse: 0.338075\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[95]\ttraining's rmse: 0.218179\tvalid_1's rmse: 0.356681\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[147]\ttraining's rmse: 0.178429\tvalid_1's rmse: 0.341759\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[127]\ttraining's rmse: 0.200492\tvalid_1's rmse: 0.35759\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[165]\ttraining's rmse: 0.169634\tvalid_1's rmse: 0.354142\n",
            "Saved submission_final_polars.csv\n",
            "Submission shape: (372399, 2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Iteration G: Ensemble (GPU-optimized with minimal transfers)\n",
        "\n",
        "print(\"Training Ensemble (Iter G)...\")\n",
        "\n",
        "horizons = sorted(train_df[\"horizon\"].unique().to_list())\n",
        "param_sets = [\n",
        "    {\"num_leaves\": 31, \"learning_rate\": 0.05, \"bagging_fraction\": 0.8},\n",
        "    {\"num_leaves\": 63, \"learning_rate\": 0.03, \"bagging_fraction\": 0.9},\n",
        "]\n",
        "\n",
        "preds_all_g = []\n",
        "\n",
        "for h in horizons:\n",
        "    t_h = train_df.filter(pl.col(\"horizon\") == h)\n",
        "    v_h = valid_df.filter(pl.col(\"horizon\") == h)\n",
        "    if t_h.height == 0: continue\n",
        "    \n",
        "    # Prepare features once on GPU\n",
        "    X_valid_np = v_h.select(selected_features_f).fill_null(0).to_numpy()\n",
        "    X_valid_gpu = cpu_to_gpu(X_valid_np)\n",
        "    \n",
        "    # Train models and collect predictions on GPU\n",
        "    horizon_preds_gpu = []\n",
        "    for p in param_sets:\n",
        "        full_params = {\"objective\": \"regression\", \"metric\": \"rmse\", \"verbose\": -1, \"n_jobs\": -1, \"device\": \"gpu\", **p}\n",
        "        model = train_lgb_model(t_h, v_h, selected_features_f, params=full_params)\n",
        "        preds = model.predict(X_valid_np)\n",
        "        horizon_preds_gpu.append(cpu_to_gpu(preds))\n",
        "    \n",
        "    # Average on GPU\n",
        "    stacked_gpu = np.stack(horizon_preds_gpu)\n",
        "    avg_preds_gpu = np.mean(stacked_gpu, axis=0)\n",
        "    avg_preds = gpu_to_cpu(avg_preds_gpu)\n",
        "    \n",
        "    # Store with ID\n",
        "    temp_df = v_h.select(\"id\").with_columns(pl.Series(\"prediction\", avg_preds))\n",
        "    preds_all_g.append(temp_df)\n",
        "\n",
        "if preds_all_g:\n",
        "    submission = pl.concat(preds_all_g)\n",
        "    submission.write_csv(\"submission_final_polars.csv\")\n",
        "    print(\"Saved submission_final_polars.csv\")\n",
        "    print(f\"Submission shape: {submission.shape}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
